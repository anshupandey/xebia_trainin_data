{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec model with gensim IMDB reviews dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/xebia_training_data/blob/main/word2vec_model_with_gensim_IMDB_reviews_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyc_0lfv98xf"
      },
      "source": [
        "# The data\n",
        "\n",
        "   ##  About the data\n",
        "The analysis seeks to establish transformation of word into vectors on any text. We are not concerned about whether the text data has label or not. The data set supplied consists of  **50000 IMDB reviews**  with review ID on a certain movie  with no labels.We'll use this unlabelled data to train a model. which can be applied on test data.\n",
        "\n",
        "Please visit the site to download the data\n",
        "https://www.kaggle.com/c/word2vec-nlp-tutorial/data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qjeipfq-Xvy"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUO7x4uTqEyv"
      },
      "source": [
        "## Import the data\n",
        "\n",
        "The data was imported from local repository using the command below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EurxAg-o7Oe3"
      },
      "source": [
        "!wget -q https://www.dropbox.com/s/0ygoimffauvl7x5/unlabeledTrainData.tsv"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb2PtvNs-ezJ"
      },
      "source": [
        "df=pd.read_csv(\"unlabeledTrainData.tsv\",delimiter=\"\\t\",quoting=3,header=0)\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beTir5Bc-e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "22f5a138-85db-4fcf-9754-9e7566e382d8"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"9999_0\"</td>\n",
              "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"45057_0\"</td>\n",
              "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"15561_0\"</td>\n",
              "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"7161_0\"</td>\n",
              "      <td>\"I went to see this film with a great deal of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"43971_0\"</td>\n",
              "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id                                             review\n",
              "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
              "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
              "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
              "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
              "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCGqHLWu-e7m"
      },
      "source": [
        "import re,string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEa71UBQe9X7"
      },
      "source": [
        "##  Data Cleaning\n",
        "We've gone through the reviews & detected punctuations in many reviews.The punctuations don't contribute anything to our analysis & moreover they are considered as unique word & distort the meaning of other words.This is why the data needs to be cleaned before we jump into core analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWxkM_PZ-e-h"
      },
      "source": [
        "def clean_string(string):                                                         # The entire document is cleaned defining clean_string\n",
        "  try:\n",
        "    string=re.sub(r'^https?:\\/\\/<>.*[\\r\\n]*','',string,flags=re.MULTILINE) # remove URLS\n",
        "    string=re.sub(r\"[^A-Za-z]\",\" \",string) # remove non alphabetic tokens\n",
        "    words=string.strip().lower().split() # removing extra space\n",
        "    return \" \".join(words)\n",
        "  except:\n",
        "    return \" \"\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWcGFms8a9TK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwzVmg29asKk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "36b86552-e85f-4431-f768-5b41ef36739b"
      },
      "source": [
        "doc = \"my name is anshu 123anshu http://www.anshi.com jeio\"\n",
        "re.sub(r\"[^A-Za-z]\",\" \",doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my name is anshu    anshu http   www anshi com jeio'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRrc9brjJyEP"
      },
      "source": [
        "Above we defined a function called **clean_string** & this function we have applied on the raw review column and created a new column(**clean_review**) to save the cleaned reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rZpIB7i-fBK"
      },
      "source": [
        "df['clean_review']=df.review.apply(clean_string)                                  # Finally cleaned format is applied on the reviews\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrIOSWx3-fFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "c652a995-18cc-400b-e481-28e45f5c4910"
      },
      "source": [
        "print (\"No.of samples \\n:\",(len(df)))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No.of samples \n",
            ": 50000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "      <th>clean_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"9999_0\"</td>\n",
              "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
              "      <td>watching time chasers it obvious that it was m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"45057_0\"</td>\n",
              "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
              "      <td>i saw this film about years ago and remember i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"15561_0\"</td>\n",
              "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
              "      <td>minor spoilers br br in new york joan barnard ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"7161_0\"</td>\n",
              "      <td>\"I went to see this film with a great deal of ...</td>\n",
              "      <td>i went to see this film with a great deal of e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"43971_0\"</td>\n",
              "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
              "      <td>yes i agree with everyone on this site this mo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                       clean_review\n",
              "0   \"9999_0\"  ...  watching time chasers it obvious that it was m...\n",
              "1  \"45057_0\"  ...  i saw this film about years ago and remember i...\n",
              "2  \"15561_0\"  ...  minor spoilers br br in new york joan barnard ...\n",
              "3   \"7161_0\"  ...  i went to see this film with a great deal of e...\n",
              "4  \"43971_0\"  ...  yes i agree with everyone on this site this mo...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEWVMOiItP2L"
      },
      "source": [
        "If we look at the data now, we'll not notice any punctuations in the **clean_review** column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcKWqrLo2ZRz"
      },
      "source": [
        "#  Word2Vec with Gensim(The Word2Vec toolkit)\n",
        "\n",
        "Gensim is an open source Python library for natural language processing, with a focus on topic modeling.Gensim was developed and is maintained by the Czech natural language processing researcher **Radim Řehůřek** and his company RaRe Technologies.\n",
        "\n",
        "It is not an everything-including-the-kitchen-sink NLP research library (like NLTK); instead, Gensim is a mature, focused, and efficient suite of NLP tools for topic modeling. Most notably for this tutorial, it supports an implementation of the** Word2Vec word embedding** for learning new word vectors from text.\n",
        "\n",
        "It also provides tools for loading pre-trained word embeddings in a few formats and for making use and querying a loaded embedding.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "In this tutorial, we dig a little \"deeper\" into sentiment analysis. Google's Word2Vec is a deep-learning inspired method that focuses on the meaning of words. Word2Vec attempts to understand meaning and **semantic relationships** among words. It works in a way that is similar to deep approaches, such as recurrent neural nets or deep neural nets, but is computationally more efficient. This tutorial focuses on Word2Vec for sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4OuxlUSuPy7"
      },
      "source": [
        "**Please install & import the gensim everytime you work on Google colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK0rq-GZ-e45"
      },
      "source": [
        "!pip install gensim --quiet                                      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16M51r6GOuFC"
      },
      "source": [
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgGRlg9MuehZ"
      },
      "source": [
        "**Since we are going to work with words, so we are required to split the each review so that we can have word tokens.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtVP0tOb-fJL"
      },
      "source": [
        "# Word tokenization\n",
        "Document=[]\n",
        "for doc in df['clean_review']:\n",
        "  Document.append(doc.split(' '))                             "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tta5zrdnrlft",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "117aafc9-a748-4f8a-cb55-e325df8cab16"
      },
      "source": [
        "len(Document)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heF-_9pObZiN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfp2mtdQu8oU"
      },
      "source": [
        "**Let us explore split reviews**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV4hRqzwVt4V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2fdc0510-eafe-4790-94a9-378cce0098cc"
      },
      "source": [
        "Document[10][6:13]                                                                # This what is there in 10th Document starting from 6 till 12"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['movie', 'i', 'am', 'not', 'sure', 'whether', 'i']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQWDFXrYVt7G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "b2280013-3cdd-48d3-d453-cb4a62cbca99"
      },
      "source": [
        "print(len(Document[10]))                                                          # Lenth of the 10th document ,  It has 524 words in it\n",
        "print(Document[10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "524\n",
            "['after', 'reading', 'the', 'comments', 'for', 'this', 'movie', 'i', 'am', 'not', 'sure', 'whether', 'i', 'should', 'be', 'angry', 'sad', 'or', 'sickened', 'seeing', 'comments', 'typical', 'of', 'people', 'who', 'a', 'know', 'absolutely', 'nothing', 'about', 'the', 'military', 'or', 'b', 'who', 'base', 'everything', 'they', 'think', 'they', 'know', 'on', 'movies', 'like', 'this', 'or', 'on', 'cnn', 'reports', 'about', 'abu', 'gharib', 'makes', 'me', 'wonder', 'about', 'the', 'state', 'of', 'intellectual', 'stimulation', 'in', 'the', 'world', 'br', 'br', 'at', 'the', 'time', 'i', 'type', 'this', 'the', 'number', 'of', 'people', 'in', 'the', 'us', 'military', 'million', 'on', 'active', 'duty', 'with', 'another', 'almost', 'in', 'the', 'guard', 'and', 'reserves', 'for', 'a', 'total', 'of', 'roughly', 'million', 'br', 'br', 'the', 'number', 'of', 'people', 'indicted', 'for', 'abuses', 'at', 'at', 'abu', 'gharib', 'currently', 'less', 'than', 'br', 'br', 'that', 'makes', 'the', 'total', 'of', 'people', 'indicted', 'of', 'the', 'total', 'military', 'even', 'if', 'you', 'indict', 'every', 'single', 'military', 'member', 'that', 'ever', 'stepped', 'in', 'to', 'abu', 'gharib', 'you', 'would', 'not', 'come', 'close', 'to', 'making', 'that', 'a', 'whole', 'number', 'br', 'br', 'the', 'flaws', 'in', 'this', 'movie', 'would', 'take', 'years', 'to', 'cover', 'i', 'understand', 'that', 'it', 's', 'supposed', 'to', 'be', 'sarcastic', 'but', 'in', 'reality', 'the', 'writer', 'and', 'director', 'are', 'trying', 'to', 'make', 'commentary', 'about', 'the', 'state', 'of', 'the', 'military', 'without', 'an', 'enemy', 'to', 'fight', 'in', 'reality', 'the', 'us', 'military', 'has', 'been', 'at', 'its', 'busiest', 'when', 'there', 'are', 'not', 'conflicts', 'going', 'on', 'the', 'military', 'is', 'the', 'first', 'called', 'for', 'disaster', 'relief', 'and', 'humanitarian', 'aid', 'missions', 'when', 'the', 'tsunami', 'hit', 'indonesia', 'devestating', 'the', 'region', 'the', 'us', 'military', 'was', 'the', 'first', 'on', 'the', 'scene', 'when', 'the', 'chaos', 'of', 'the', 'situation', 'overwhelmed', 'the', 'local', 'governments', 'it', 'was', 'military', 'leadership', 'who', 'looked', 'at', 'their', 'people', 'the', 'same', 'people', 'this', 'movie', 'mocks', 'and', 'said', 'make', 'it', 'happen', 'within', 'hours', 'food', 'aid', 'was', 'reaching', 'isolated', 'villages', 'within', 'days', 'airfields', 'were', 'built', 'cargo', 'aircraft', 'started', 'landing', 'and', 'a', 'food', 'distribution', 'system', 'was', 'up', 'and', 'running', 'hours', 'and', 'days', 'not', 'weeks', 'and', 'months', 'yes', 'there', 'are', 'unscrupulous', 'people', 'in', 'the', 'us', 'military', 'but', 'then', 'there', 'are', 'in', 'every', 'walk', 'of', 'life', 'every', 'occupation', 'but', 'to', 'see', 'people', 'on', 'this', 'website', 'decide', 'that', 'million', 'men', 'and', 'women', 'are', 'all', 'criminal', 'with', 'nothing', 'on', 'their', 'minds', 'but', 'thoughts', 'of', 'destruction', 'or', 'mayhem', 'is', 'an', 'absolute', 'disservice', 'to', 'the', 'things', 'that', 'they', 'do', 'every', 'day', 'one', 'person', 'on', 'this', 'website', 'even', 'went', 'so', 'far', 'as', 'to', 'say', 'that', 'military', 'members', 'are', 'in', 'it', 'for', 'personal', 'gain', 'wow', 'entry', 'level', 'personnel', 'make', 'just', 'under', 'an', 'hour', 'assuming', 'a', 'hour', 'work', 'week', 'of', 'course', 'many', 'work', 'much', 'more', 'than', 'hours', 'a', 'week', 'and', 'those', 'in', 'harm', 's', 'way', 'typically', 'put', 'in', 'hour', 'days', 'for', 'months', 'on', 'end', 'that', 'makes', 'the', 'pay', 'well', 'under', 'minimum', 'wage', 'so', 'much', 'for', 'personal', 'gain', 'i', 'beg', 'you', 'please', 'make', 'yourself', 'familiar', 'with', 'the', 'world', 'around', 'you', 'go', 'to', 'a', 'nearby', 'base', 'get', 'a', 'visitor', 'pass', 'and', 'meet', 'some', 'of', 'the', 'men', 'and', 'women', 'you', 'are', 'so', 'quick', 'to', 'disparage', 'you', 'would', 'be', 'surprised', 'the', 'military', 'no', 'longer', 'accepts', 'people', 'in', 'lieu', 'of', 'prison', 'time', 'they', 'require', 'a', 'minimum', 'of', 'a', 'ged', 'and', 'prefer', 'a', 'high', 'school', 'diploma', 'the', 'middle', 'ranks', 'are', 'expected', 'to', 'get', 'a', 'minimum', 'of', 'undergraduate', 'degrees', 'and', 'the', 'upper', 'ranks', 'are', 'encouraged', 'to', 'get', 'advanced', 'degrees']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jamR9vRCPJM-"
      },
      "source": [
        "import logging                                                                    # Please import logging to keep & check information regarding word2vec transformation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fdKWAs2VuAX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "554ab685-13fd-47c6-b1f4-d26795b619be"
      },
      "source": [
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "model=gensim.models.Word2Vec(Document,                                           # List of reviews\n",
        "                          min_count=10,                                          # we want words appearing atleast 10 times in the vocab otherwise ignore min_df\n",
        "                          workers=4,                                             # Use these many worker threads to train the model (=faster training with multicore machines\n",
        "                           size=50,                                              # it means aword is represented by 50 numbers,in other words the number of neorons in hidden layer is 50 \n",
        "                          window=5)                                              # 5 neighbors on the either side of a word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-19 05:38:10,237 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
            "2020-01-19 05:38:10,241 : INFO : collecting all words and their counts\n",
            "2020-01-19 05:38:10,243 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2020-01-19 05:38:10,856 : INFO : PROGRESS: at sentence #10000, processed 2399440 words, keeping 51654 word types\n",
            "2020-01-19 05:38:11,556 : INFO : PROGRESS: at sentence #20000, processed 4835846 words, keeping 69077 word types\n",
            "2020-01-19 05:38:12,247 : INFO : PROGRESS: at sentence #30000, processed 7267977 words, keeping 81515 word types\n",
            "2020-01-19 05:38:12,937 : INFO : PROGRESS: at sentence #40000, processed 9669772 words, keeping 91685 word types\n",
            "2020-01-19 05:38:13,643 : INFO : collected 100479 word types from a corpus of 12084660 raw words and 50000 sentences\n",
            "2020-01-19 05:38:13,644 : INFO : Loading a fresh vocabulary\n",
            "2020-01-19 05:38:14,166 : INFO : effective_min_count=10 retains 28322 unique words (28% of original 100479, drops 72157)\n",
            "2020-01-19 05:38:14,167 : INFO : effective_min_count=10 leaves 11910457 word corpus (98% of original 12084660, drops 174203)\n",
            "2020-01-19 05:38:14,282 : INFO : deleting the raw counts dictionary of 100479 items\n",
            "2020-01-19 05:38:14,288 : INFO : sample=0.001 downsamples 49 most-common words\n",
            "2020-01-19 05:38:14,290 : INFO : downsampling leaves estimated 8817283 word corpus (74.0% of prior 11910457)\n",
            "2020-01-19 05:38:14,461 : INFO : estimated required memory for 28322 words and 50 dimensions: 25489800 bytes\n",
            "2020-01-19 05:38:14,464 : INFO : resetting layer weights\n",
            "2020-01-19 05:38:20,404 : INFO : training model with 4 workers on 28322 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2020-01-19 05:38:21,416 : INFO : EPOCH 1 - PROGRESS: at 4.51% examples, 398736 words/s, in_qsize 5, out_qsize 2\n",
            "2020-01-19 05:38:22,441 : INFO : EPOCH 1 - PROGRESS: at 9.42% examples, 406311 words/s, in_qsize 8, out_qsize 1\n",
            "2020-01-19 05:38:23,496 : INFO : EPOCH 1 - PROGRESS: at 14.37% examples, 407078 words/s, in_qsize 8, out_qsize 2\n",
            "2020-01-19 05:38:24,500 : INFO : EPOCH 1 - PROGRESS: at 19.10% examples, 408637 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:38:25,501 : INFO : EPOCH 1 - PROGRESS: at 23.99% examples, 412733 words/s, in_qsize 8, out_qsize 1\n",
            "2020-01-19 05:38:26,513 : INFO : EPOCH 1 - PROGRESS: at 28.38% examples, 407682 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:27,534 : INFO : EPOCH 1 - PROGRESS: at 33.19% examples, 409587 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:28,545 : INFO : EPOCH 1 - PROGRESS: at 37.61% examples, 406988 words/s, in_qsize 5, out_qsize 2\n",
            "2020-01-19 05:38:29,574 : INFO : EPOCH 1 - PROGRESS: at 42.41% examples, 408153 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:30,597 : INFO : EPOCH 1 - PROGRESS: at 47.19% examples, 409357 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:38:31,613 : INFO : EPOCH 1 - PROGRESS: at 52.24% examples, 411337 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:32,660 : INFO : EPOCH 1 - PROGRESS: at 57.00% examples, 410835 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:33,674 : INFO : EPOCH 1 - PROGRESS: at 61.57% examples, 410268 words/s, in_qsize 5, out_qsize 2\n",
            "2020-01-19 05:38:34,699 : INFO : EPOCH 1 - PROGRESS: at 66.53% examples, 411933 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:35,711 : INFO : EPOCH 1 - PROGRESS: at 71.26% examples, 411423 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:36,758 : INFO : EPOCH 1 - PROGRESS: at 75.92% examples, 410355 words/s, in_qsize 7, out_qsize 1\n",
            "2020-01-19 05:38:37,769 : INFO : EPOCH 1 - PROGRESS: at 80.86% examples, 410869 words/s, in_qsize 8, out_qsize 0\n",
            "2020-01-19 05:38:38,781 : INFO : EPOCH 1 - PROGRESS: at 85.69% examples, 411320 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:39,836 : INFO : EPOCH 1 - PROGRESS: at 90.60% examples, 411155 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:40,865 : INFO : EPOCH 1 - PROGRESS: at 95.39% examples, 411410 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:41,730 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-01-19 05:38:41,755 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-01-19 05:38:41,768 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-01-19 05:38:41,774 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-01-19 05:38:41,775 : INFO : EPOCH - 1 : training on 12084660 raw words (8818424 effective words) took 21.4s, 412756 effective words/s\n",
            "2020-01-19 05:38:42,784 : INFO : EPOCH 2 - PROGRESS: at 4.69% examples, 413603 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:38:43,838 : INFO : EPOCH 2 - PROGRESS: at 9.83% examples, 418102 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:38:44,900 : INFO : EPOCH 2 - PROGRESS: at 15.03% examples, 420782 words/s, in_qsize 5, out_qsize 2\n",
            "2020-01-19 05:38:45,904 : INFO : EPOCH 2 - PROGRESS: at 19.91% examples, 422278 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:46,912 : INFO : EPOCH 2 - PROGRESS: at 24.52% examples, 418921 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:47,936 : INFO : EPOCH 2 - PROGRESS: at 29.16% examples, 415662 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:48,942 : INFO : EPOCH 2 - PROGRESS: at 33.96% examples, 416217 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:49,970 : INFO : EPOCH 2 - PROGRESS: at 38.67% examples, 415434 words/s, in_qsize 5, out_qsize 2\n",
            "2020-01-19 05:38:50,997 : INFO : EPOCH 2 - PROGRESS: at 43.48% examples, 415717 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:51,998 : INFO : EPOCH 2 - PROGRESS: at 48.13% examples, 415637 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:53,021 : INFO : EPOCH 2 - PROGRESS: at 52.88% examples, 414910 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:38:54,035 : INFO : EPOCH 2 - PROGRESS: at 57.57% examples, 414643 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:38:55,057 : INFO : EPOCH 2 - PROGRESS: at 62.27% examples, 414580 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:56,065 : INFO : EPOCH 2 - PROGRESS: at 67.02% examples, 414857 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:57,103 : INFO : EPOCH 2 - PROGRESS: at 71.90% examples, 414423 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:38:58,125 : INFO : EPOCH 2 - PROGRESS: at 76.33% examples, 412158 words/s, in_qsize 7, out_qsize 1\n",
            "2020-01-19 05:38:59,159 : INFO : EPOCH 2 - PROGRESS: at 81.52% examples, 413568 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:00,160 : INFO : EPOCH 2 - PROGRESS: at 86.36% examples, 414110 words/s, in_qsize 8, out_qsize 0\n",
            "2020-01-19 05:39:01,191 : INFO : EPOCH 2 - PROGRESS: at 91.26% examples, 414293 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:39:02,210 : INFO : EPOCH 2 - PROGRESS: at 96.10% examples, 414929 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:02,984 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-01-19 05:39:02,995 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-01-19 05:39:03,009 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-01-19 05:39:03,019 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-01-19 05:39:03,020 : INFO : EPOCH - 2 : training on 12084660 raw words (8816206 effective words) took 21.2s, 415042 effective words/s\n",
            "2020-01-19 05:39:04,045 : INFO : EPOCH 3 - PROGRESS: at 4.60% examples, 400975 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:05,052 : INFO : EPOCH 3 - PROGRESS: at 9.35% examples, 403662 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:06,078 : INFO : EPOCH 3 - PROGRESS: at 14.30% examples, 409038 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:07,109 : INFO : EPOCH 3 - PROGRESS: at 19.20% examples, 410927 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:08,139 : INFO : EPOCH 3 - PROGRESS: at 23.70% examples, 405153 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:39:09,141 : INFO : EPOCH 3 - PROGRESS: at 28.45% examples, 407844 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:10,144 : INFO : EPOCH 3 - PROGRESS: at 33.12% examples, 408786 words/s, in_qsize 8, out_qsize 1\n",
            "2020-01-19 05:39:11,144 : INFO : EPOCH 3 - PROGRESS: at 37.86% examples, 410352 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:12,149 : INFO : EPOCH 3 - PROGRESS: at 42.58% examples, 411480 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:13,158 : INFO : EPOCH 3 - PROGRESS: at 47.27% examples, 412192 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:14,162 : INFO : EPOCH 3 - PROGRESS: at 52.09% examples, 412448 words/s, in_qsize 8, out_qsize 0\n",
            "2020-01-19 05:39:15,172 : INFO : EPOCH 3 - PROGRESS: at 56.68% examples, 411861 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:16,196 : INFO : EPOCH 3 - PROGRESS: at 61.26% examples, 410991 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:17,198 : INFO : EPOCH 3 - PROGRESS: at 65.91% examples, 411246 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:18,207 : INFO : EPOCH 3 - PROGRESS: at 70.71% examples, 411309 words/s, in_qsize 8, out_qsize 2\n",
            "2020-01-19 05:39:19,229 : INFO : EPOCH 3 - PROGRESS: at 75.24% examples, 410108 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:20,236 : INFO : EPOCH 3 - PROGRESS: at 80.16% examples, 410689 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:21,264 : INFO : EPOCH 3 - PROGRESS: at 85.07% examples, 411546 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:22,282 : INFO : EPOCH 3 - PROGRESS: at 90.00% examples, 412095 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:23,287 : INFO : EPOCH 3 - PROGRESS: at 94.68% examples, 412103 words/s, in_qsize 7, out_qsize 1\n",
            "2020-01-19 05:39:24,334 : INFO : EPOCH 3 - PROGRESS: at 99.57% examples, 412017 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:39:24,349 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-01-19 05:39:24,370 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-01-19 05:39:24,383 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-01-19 05:39:24,391 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-01-19 05:39:24,392 : INFO : EPOCH - 3 : training on 12084660 raw words (8816943 effective words) took 21.4s, 412681 effective words/s\n",
            "2020-01-19 05:39:25,398 : INFO : EPOCH 4 - PROGRESS: at 4.60% examples, 407925 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:26,404 : INFO : EPOCH 4 - PROGRESS: at 9.35% examples, 407524 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:27,421 : INFO : EPOCH 4 - PROGRESS: at 14.21% examples, 410690 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:28,435 : INFO : EPOCH 4 - PROGRESS: at 19.10% examples, 413738 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:39:29,447 : INFO : EPOCH 4 - PROGRESS: at 23.53% examples, 407497 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:39:30,460 : INFO : EPOCH 4 - PROGRESS: at 28.31% examples, 409215 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:31,467 : INFO : EPOCH 4 - PROGRESS: at 32.89% examples, 408586 words/s, in_qsize 8, out_qsize 1\n",
            "2020-01-19 05:39:32,478 : INFO : EPOCH 4 - PROGRESS: at 37.76% examples, 411495 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:33,490 : INFO : EPOCH 4 - PROGRESS: at 42.17% examples, 409051 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:34,495 : INFO : EPOCH 4 - PROGRESS: at 46.86% examples, 410137 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:35,497 : INFO : EPOCH 4 - PROGRESS: at 51.46% examples, 409316 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:36,502 : INFO : EPOCH 4 - PROGRESS: at 55.94% examples, 408075 words/s, in_qsize 4, out_qsize 3\n",
            "2020-01-19 05:39:37,513 : INFO : EPOCH 4 - PROGRESS: at 60.34% examples, 406758 words/s, in_qsize 8, out_qsize 3\n",
            "2020-01-19 05:39:38,557 : INFO : EPOCH 4 - PROGRESS: at 65.15% examples, 407112 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:39,583 : INFO : EPOCH 4 - PROGRESS: at 69.69% examples, 405587 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:39:40,584 : INFO : EPOCH 4 - PROGRESS: at 74.59% examples, 407034 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:39:41,587 : INFO : EPOCH 4 - PROGRESS: at 79.59% examples, 408296 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:42,637 : INFO : EPOCH 4 - PROGRESS: at 84.47% examples, 408787 words/s, in_qsize 5, out_qsize 2\n",
            "2020-01-19 05:39:43,668 : INFO : EPOCH 4 - PROGRESS: at 89.61% examples, 409959 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:44,677 : INFO : EPOCH 4 - PROGRESS: at 94.42% examples, 410722 words/s, in_qsize 8, out_qsize 0\n",
            "2020-01-19 05:39:45,712 : INFO : EPOCH 4 - PROGRESS: at 99.32% examples, 410928 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:45,829 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-01-19 05:39:45,834 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-01-19 05:39:45,839 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-01-19 05:39:45,840 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-01-19 05:39:45,841 : INFO : EPOCH - 4 : training on 12084660 raw words (8818770 effective words) took 21.4s, 411238 effective words/s\n",
            "2020-01-19 05:39:46,884 : INFO : EPOCH 5 - PROGRESS: at 4.70% examples, 399589 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:47,911 : INFO : EPOCH 5 - PROGRESS: at 9.58% examples, 406286 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:48,912 : INFO : EPOCH 5 - PROGRESS: at 14.47% examples, 411774 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:49,930 : INFO : EPOCH 5 - PROGRESS: at 19.02% examples, 407223 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:50,963 : INFO : EPOCH 5 - PROGRESS: at 23.53% examples, 401944 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:39:51,985 : INFO : EPOCH 5 - PROGRESS: at 28.31% examples, 403897 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:52,998 : INFO : EPOCH 5 - PROGRESS: at 32.87% examples, 403844 words/s, in_qsize 7, out_qsize 1\n",
            "2020-01-19 05:39:54,026 : INFO : EPOCH 5 - PROGRESS: at 37.43% examples, 402901 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:55,051 : INFO : EPOCH 5 - PROGRESS: at 42.11% examples, 403187 words/s, in_qsize 8, out_qsize 1\n",
            "2020-01-19 05:39:56,067 : INFO : EPOCH 5 - PROGRESS: at 46.62% examples, 403031 words/s, in_qsize 6, out_qsize 2\n",
            "2020-01-19 05:39:57,116 : INFO : EPOCH 5 - PROGRESS: at 51.54% examples, 403681 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:58,125 : INFO : EPOCH 5 - PROGRESS: at 56.24% examples, 404553 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:39:59,149 : INFO : EPOCH 5 - PROGRESS: at 60.92% examples, 404768 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:40:00,206 : INFO : EPOCH 5 - PROGRESS: at 65.65% examples, 404419 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:40:01,246 : INFO : EPOCH 5 - PROGRESS: at 70.39% examples, 403626 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:40:02,264 : INFO : EPOCH 5 - PROGRESS: at 74.98% examples, 403419 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:40:03,269 : INFO : EPOCH 5 - PROGRESS: at 79.74% examples, 403615 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:40:04,274 : INFO : EPOCH 5 - PROGRESS: at 84.39% examples, 404181 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-19 05:40:05,291 : INFO : EPOCH 5 - PROGRESS: at 89.40% examples, 405148 words/s, in_qsize 8, out_qsize 1\n",
            "2020-01-19 05:40:06,304 : INFO : EPOCH 5 - PROGRESS: at 94.35% examples, 406768 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:40:07,329 : INFO : EPOCH 5 - PROGRESS: at 99.08% examples, 406659 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-19 05:40:07,457 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-01-19 05:40:07,474 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-01-19 05:40:07,489 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-01-19 05:40:07,497 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-01-19 05:40:07,499 : INFO : EPOCH - 5 : training on 12084660 raw words (8817951 effective words) took 21.7s, 407217 effective words/s\n",
            "2020-01-19 05:40:07,502 : INFO : training on a 60423300 raw words (44088294 effective words) took 107.1s, 411670 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHlVJEoXC5PD"
      },
      "source": [
        "**Please note that after applying Word2Vec function on the clean_review giving all the arguments corretly we have got 28322 words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RYnV85LCq3r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d822c2b1-6f27-4138-b0bb-f0e876e399c7"
      },
      "source": [
        "print(len(model.wv.vocab))                                                        # Now the vocab contains 28322 uinque words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdk_HDQWD0ct"
      },
      "source": [
        "**Let's check the dimension of a vector i.e. the number of words that represent a word**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DGahs80Dv6J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "51c92d14-7b67-425e-ec0e-4abebf7cf9a6"
      },
      "source": [
        "print(model.wv.vector_size)                                                       # It means each vector has 50 numbers in it or in other words each word is vector of 5o numbers that we predefined"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqmrT73MQsgm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "33c535f3-76f8-4a0c-dab7-1d3d1d4e9d77"
      },
      "source": [
        "model.wv.vectors.shape                                                            # Dimension of the the entire corpus        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28322, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyTej6dydwW6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "45032142-cc50-47c9-c4a9-c1721166221f"
      },
      "source": [
        "model['beautiful'].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhGHd2OREPSz"
      },
      "source": [
        "### Let's explore some interesting results of word2vec experiment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db44U_SoVuC6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "01346be3-b4b1-48e3-a9b8-fdd8efd76f06"
      },
      "source": [
        "model.wv.most_similar(\"beautiful\")                                                # 10 similar words beautiful,the maximum similarity is 1,minimum is 0.When they are completely similar the \n",
        "                                                                                  # Value will be 1 , when completely dissimilar,the value will be 0."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-19 05:40:07,572 : INFO : precomputing L2-norms of word weight vectors\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('gorgeous', 0.8449814319610596),\n",
              " ('lovely', 0.8257693648338318),\n",
              " ('stunning', 0.819052517414093),\n",
              " ('haunting', 0.7595319747924805),\n",
              " ('wonderful', 0.7336277365684509),\n",
              " ('delightful', 0.7012267112731934),\n",
              " ('breathtaking', 0.6977381706237793),\n",
              " ('vibrant', 0.6946560740470886),\n",
              " ('exquisite', 0.6888635754585266),\n",
              " ('fabulous', 0.6762886047363281)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NbHKeQg6kP9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "352b71f2-a4ef-46eb-9e9b-7113c3674bc5"
      },
      "source": [
        "model.wv.most_similar(\"princess\")                                                  # 10 similar words returned with numbers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('maid', 0.8642414808273315),\n",
              " ('widow', 0.8594434857368469),\n",
              " ('nurse', 0.8591358661651611),\n",
              " ('belle', 0.8179522156715393),\n",
              " ('maria', 0.7995132207870483),\n",
              " ('prince', 0.7979990243911743),\n",
              " ('prostitute', 0.7876790165901184),\n",
              " ('aunt', 0.7868121862411499),\n",
              " ('alice', 0.7834028005599976),\n",
              " ('temple', 0.7816338539123535)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-tDNwJD-fQs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "5c8e2fe1-37ab-4c95-bb94-77aa91c2616c"
      },
      "source": [
        "model.wv.doesnt_match(\"she talked to me in the evening publicly alice\".split())         # publicly does not match in the sentence given"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'alice'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3lClNUZFpQE"
      },
      "source": [
        "Below the word **right** is represented by a dense 50 dimensional vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sj-zNbJ6kKo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "ef92ba35-9b51-4e0b-b3aa-3c873ad0df18"
      },
      "source": [
        "model.wv[\"right\"]                                                                  # right word is represented by 50 numbers in other words the word \"right\" is vector of 50 numbers\n",
        "                                                                                   # 50 numbers are summarized weights because these numbers are obtained in the hidden layer of predefined 50 neurons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.6846868 , -0.7968005 , -0.6302324 , -3.394566  ,  0.7201638 ,\n",
              "        1.1136231 ,  0.15784374,  2.5323277 ,  3.5603287 ,  0.59281504,\n",
              "       -0.9188659 , -0.447955  ,  0.62603706, -0.03631172,  1.1669568 ,\n",
              "        1.2037969 , -1.878727  ,  1.026431  ,  1.2620952 ,  0.87099075,\n",
              "       -0.40098897, -2.2058098 , -2.5231655 , -0.3569199 , -1.5878537 ,\n",
              "       -1.0750184 ,  0.15629587, -0.23911397, -1.4470414 , -0.1245377 ,\n",
              "        1.136354  , -0.6609408 ,  0.5265115 ,  0.6114063 , -0.15428022,\n",
              "       -0.1421413 , -0.569659  ,  2.165041  , -2.8660035 ,  0.5492531 ,\n",
              "        0.93151784,  0.21207607,  2.0477517 ,  1.5115618 ,  0.76157683,\n",
              "       -1.3251568 ,  0.78988296,  0.98710644,  1.5385407 ,  3.4042273 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnENAcilu0pP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "477da5bc-bb26-4c26-cebc-15f854841fab"
      },
      "source": [
        "model.wv['great']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.4401122 ,  0.76371634, -1.4465237 , -2.4345956 ,  1.4818008 ,\n",
              "        2.3014462 ,  0.35223305, -0.28248826,  1.3010585 ,  0.46277562,\n",
              "       -0.43015203,  0.64026016,  0.26170298,  0.63460237,  2.4360445 ,\n",
              "        1.3040127 ,  1.5610905 , -2.5930145 , -0.68386453,  1.2764063 ,\n",
              "       -2.0910609 , -0.79727757, -1.6921145 ,  0.81008387, -1.4320858 ,\n",
              "        2.696001  , -0.83759457, -1.203519  ,  0.7344471 ,  4.284749  ,\n",
              "       -0.97111064, -0.95781547,  5.1075478 , -0.13110286, -1.0417333 ,\n",
              "        1.0495613 ,  0.5642384 ,  4.2558985 , -3.4282224 , -0.5562852 ,\n",
              "        2.6257257 ,  0.06977661, -0.5748929 ,  2.0517044 ,  1.0178621 ,\n",
              "       -0.41138136, -1.5831146 ,  2.879354  , -0.30539533,  2.2108846 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWdfqY5QuPRa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "4c4b89d7-4535-4979-8ea8-f68363de9d41"
      },
      "source": [
        "model.wv."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-518bbf67b2e3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    model.wv.\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CItdPnnHGk66"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MMsUOIp-fNX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "8febec39-9223-4ce7-84be-5724ce534a70"
      },
      "source": [
        "model.save(\"word2vec movie-50\")                                                    # We save this model for further use.\n",
        "                                                                                   # Google has such many pre-trained models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-19 05:41:08,396 : INFO : saving Word2Vec object under word2vec movie-50, separately None\n",
            "2020-01-19 05:41:08,398 : INFO : not storing attribute vectors_norm\n",
            "2020-01-19 05:41:08,404 : INFO : not storing attribute cum_table\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "2020-01-19 05:41:08,692 : INFO : saved word2vec movie-50\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a1G2kw_6kf2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAVyObUcLqV9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}