{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "code - Text generation - word_sequence_prediction.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/xebia_training_data/blob/main/code_Text_generation_word_sequence_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcdbK72a_X-F"
      },
      "source": [
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAUNHhd__X-H",
        "outputId": "8d8aac46-a28c-40c2-c60c-0273287c3174"
      },
      "source": [
        "data = \"\"\" Jack and Jill went up the hill\n",
        "To fetch a pail of water\n",
        "Jack fell down and broke his crown\n",
        "And Jill came tumbling after\"\"\"\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Jack and Jill went up the hill\n",
            "To fetch a pail of water\n",
            "Jack fell down and broke his crown\n",
            "And Jill came tumbling after\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzQ03dmu_X-J",
        "outputId": "b0c7f230-2f2f-453b-ee49-64d1819d8417"
      },
      "source": [
        "# integer encode text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded = tokenizer.texts_to_sequences([data])[0]\n",
        "print(encoded)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 2, 14, 15, 1, 16, 17, 18, 1, 3, 19, 20, 21]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCyERZg6_X-J",
        "outputId": "aa9b420d-496b-4ab9-edd9-562ce67fa098"
      },
      "source": [
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_index.items())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'and': 1, 'jack': 2, 'jill': 3, 'went': 4, 'up': 5, 'the': 6, 'hill': 7, 'to': 8, 'fetch': 9, 'a': 10, 'pail': 11, 'of': 12, 'water': 13, 'fell': 14, 'down': 15, 'broke': 16, 'his': 17, 'crown': 18, 'came': 19, 'tumbling': 20, 'after': 21}\n",
            "dict_items([('and', 1), ('jack', 2), ('jill', 3), ('went', 4), ('up', 5), ('the', 6), ('hill', 7), ('to', 8), ('fetch', 9), ('a', 10), ('pail', 11), ('of', 12), ('water', 13), ('fell', 14), ('down', 15), ('broke', 16), ('his', 17), ('crown', 18), ('came', 19), ('tumbling', 20), ('after', 21)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXqwCDSs_X-K",
        "outputId": "0dd5f82b-fb81-4474-946b-83f9bd3f3d46"
      },
      "source": [
        "# determine the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOm0D_Hd_X-L",
        "outputId": "c7bc1c45-bf40-4549-e22e-9c8935542105"
      },
      "source": [
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 2, 14, 15, 1, 16, 17, 18, 1, 3, 19, 20, 21]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzmUVA26_X-L",
        "outputId": "ced30c3f-e3ac-48d0-98a1-916ede017a91"
      },
      "source": [
        "# create word -> word sequences\n",
        "sequences = list()\n",
        "for i in range(1, len(encoded)):\n",
        "    sequence = encoded[i-1:i+1]\n",
        "    sequences.append(sequence)\n",
        "print('Total Sequences: %d' % len(sequences))\n",
        "sequences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 1],\n",
              " [1, 3],\n",
              " [3, 4],\n",
              " [4, 5],\n",
              " [5, 6],\n",
              " [6, 7],\n",
              " [7, 8],\n",
              " [8, 9],\n",
              " [9, 10],\n",
              " [10, 11],\n",
              " [11, 12],\n",
              " [12, 13],\n",
              " [13, 2],\n",
              " [2, 14],\n",
              " [14, 15],\n",
              " [15, 1],\n",
              " [1, 16],\n",
              " [16, 17],\n",
              " [17, 18],\n",
              " [18, 1],\n",
              " [1, 3],\n",
              " [3, 19],\n",
              " [19, 20],\n",
              " [20, 21]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp2t8DbZ_X-L"
      },
      "source": [
        "# split into X and y elements\n",
        "sequences = array(sequences)\n",
        "X, Y = sequences[:,0],sequences[:,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNAZcvG5_X-M",
        "outputId": "e399560c-8b13-45cf-877a-bb36e2d9014d"
      },
      "source": [
        "info = '''\n",
        "We will fit our model to predict a probability distribution across all words in the vocabulary. \n",
        "That means that we need to turn the output element from a single integer into a one hot encoding with a 0 for every word in the vocabulary and a 1 for the actual word that the value. This gives the network a ground truth to aim for from which we can calculate error and update the model.\n",
        "\n",
        "Keras provides the to_categorical() function that we can use to convert the integer to a one hot encoding while specifying the number of classes as the vocabulary size.\n",
        "'''\n",
        "\n",
        "print(info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "We will fit our model to predict a probability distribution across all words in the vocabulary. \n",
            "That means that we need to turn the output element from a single integer into a one hot encoding with a 0 for every word in the vocabulary and a 1 for the actual word that the value. This gives the network a ground truth to aim for from which we can calculate error and update the model.\n",
            "\n",
            "Keras provides the to_categorical() function that we can use to convert the integer to a one hot encoding while specifying the number of classes as the vocabulary size.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S8nAo71_X-M",
        "outputId": "fae58dca-613b-4e4f-f0b8-76f88c84cafe"
      },
      "source": [
        "# one hot encode outputs\n",
        "y = to_categorical(Y, num_classes=vocab_size)\n",
        "print(y.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(24, 22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkYcbqeZ_X-N",
        "outputId": "93ddbfbc-4af0-42e3-95fc-dea5a2931899"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(X, y, epochs=500)\n",
        "# evaluate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1, 10)             220       \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 50)                12200     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 22)                1122      \n",
            "=================================================================\n",
            "Total params: 13,542\n",
            "Trainable params: 13,542\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/500\n",
            "1/1 - 2s - loss: 3.0917 - accuracy: 0.0000e+00\n",
            "Epoch 2/500\n",
            "1/1 - 0s - loss: 3.0909 - accuracy: 0.1250\n",
            "Epoch 3/500\n",
            "1/1 - 0s - loss: 3.0901 - accuracy: 0.1250\n",
            "Epoch 4/500\n",
            "1/1 - 0s - loss: 3.0893 - accuracy: 0.1250\n",
            "Epoch 5/500\n",
            "1/1 - 0s - loss: 3.0885 - accuracy: 0.1250\n",
            "Epoch 6/500\n",
            "1/1 - 0s - loss: 3.0877 - accuracy: 0.1250\n",
            "Epoch 7/500\n",
            "1/1 - 0s - loss: 3.0868 - accuracy: 0.1250\n",
            "Epoch 8/500\n",
            "1/1 - 0s - loss: 3.0860 - accuracy: 0.1250\n",
            "Epoch 9/500\n",
            "1/1 - 0s - loss: 3.0852 - accuracy: 0.1250\n",
            "Epoch 10/500\n",
            "1/1 - 0s - loss: 3.0844 - accuracy: 0.1250\n",
            "Epoch 11/500\n",
            "1/1 - 0s - loss: 3.0835 - accuracy: 0.1250\n",
            "Epoch 12/500\n",
            "1/1 - 0s - loss: 3.0827 - accuracy: 0.1250\n",
            "Epoch 13/500\n",
            "1/1 - 0s - loss: 3.0818 - accuracy: 0.1250\n",
            "Epoch 14/500\n",
            "1/1 - 0s - loss: 3.0810 - accuracy: 0.1250\n",
            "Epoch 15/500\n",
            "1/1 - 0s - loss: 3.0801 - accuracy: 0.1250\n",
            "Epoch 16/500\n",
            "1/1 - 0s - loss: 3.0792 - accuracy: 0.1250\n",
            "Epoch 17/500\n",
            "1/1 - 0s - loss: 3.0783 - accuracy: 0.1250\n",
            "Epoch 18/500\n",
            "1/1 - 0s - loss: 3.0773 - accuracy: 0.1250\n",
            "Epoch 19/500\n",
            "1/1 - 0s - loss: 3.0764 - accuracy: 0.1250\n",
            "Epoch 20/500\n",
            "1/1 - 0s - loss: 3.0754 - accuracy: 0.1250\n",
            "Epoch 21/500\n",
            "1/1 - 0s - loss: 3.0744 - accuracy: 0.1250\n",
            "Epoch 22/500\n",
            "1/1 - 0s - loss: 3.0734 - accuracy: 0.1250\n",
            "Epoch 23/500\n",
            "1/1 - 0s - loss: 3.0723 - accuracy: 0.1250\n",
            "Epoch 24/500\n",
            "1/1 - 0s - loss: 3.0712 - accuracy: 0.1250\n",
            "Epoch 25/500\n",
            "1/1 - 0s - loss: 3.0701 - accuracy: 0.1250\n",
            "Epoch 26/500\n",
            "1/1 - 0s - loss: 3.0690 - accuracy: 0.1250\n",
            "Epoch 27/500\n",
            "1/1 - 0s - loss: 3.0679 - accuracy: 0.1250\n",
            "Epoch 28/500\n",
            "1/1 - 0s - loss: 3.0667 - accuracy: 0.1250\n",
            "Epoch 29/500\n",
            "1/1 - 0s - loss: 3.0654 - accuracy: 0.1250\n",
            "Epoch 30/500\n",
            "1/1 - 0s - loss: 3.0642 - accuracy: 0.1250\n",
            "Epoch 31/500\n",
            "1/1 - 0s - loss: 3.0629 - accuracy: 0.1250\n",
            "Epoch 32/500\n",
            "1/1 - 0s - loss: 3.0616 - accuracy: 0.1250\n",
            "Epoch 33/500\n",
            "1/1 - 0s - loss: 3.0602 - accuracy: 0.1250\n",
            "Epoch 34/500\n",
            "1/1 - 0s - loss: 3.0588 - accuracy: 0.1250\n",
            "Epoch 35/500\n",
            "1/1 - 0s - loss: 3.0574 - accuracy: 0.1250\n",
            "Epoch 36/500\n",
            "1/1 - 0s - loss: 3.0559 - accuracy: 0.1250\n",
            "Epoch 37/500\n",
            "1/1 - 0s - loss: 3.0543 - accuracy: 0.1250\n",
            "Epoch 38/500\n",
            "1/1 - 0s - loss: 3.0528 - accuracy: 0.1250\n",
            "Epoch 39/500\n",
            "1/1 - 0s - loss: 3.0511 - accuracy: 0.1250\n",
            "Epoch 40/500\n",
            "1/1 - 0s - loss: 3.0495 - accuracy: 0.1250\n",
            "Epoch 41/500\n",
            "1/1 - 0s - loss: 3.0477 - accuracy: 0.1250\n",
            "Epoch 42/500\n",
            "1/1 - 0s - loss: 3.0460 - accuracy: 0.1250\n",
            "Epoch 43/500\n",
            "1/1 - 0s - loss: 3.0441 - accuracy: 0.1250\n",
            "Epoch 44/500\n",
            "1/1 - 0s - loss: 3.0422 - accuracy: 0.1250\n",
            "Epoch 45/500\n",
            "1/1 - 0s - loss: 3.0403 - accuracy: 0.1250\n",
            "Epoch 46/500\n",
            "1/1 - 0s - loss: 3.0383 - accuracy: 0.1250\n",
            "Epoch 47/500\n",
            "1/1 - 0s - loss: 3.0362 - accuracy: 0.1250\n",
            "Epoch 48/500\n",
            "1/1 - 0s - loss: 3.0341 - accuracy: 0.1250\n",
            "Epoch 49/500\n",
            "1/1 - 0s - loss: 3.0319 - accuracy: 0.1250\n",
            "Epoch 50/500\n",
            "1/1 - 0s - loss: 3.0296 - accuracy: 0.1250\n",
            "Epoch 51/500\n",
            "1/1 - 0s - loss: 3.0273 - accuracy: 0.1250\n",
            "Epoch 52/500\n",
            "1/1 - 0s - loss: 3.0249 - accuracy: 0.1250\n",
            "Epoch 53/500\n",
            "1/1 - 0s - loss: 3.0224 - accuracy: 0.1250\n",
            "Epoch 54/500\n",
            "1/1 - 0s - loss: 3.0198 - accuracy: 0.1250\n",
            "Epoch 55/500\n",
            "1/1 - 0s - loss: 3.0172 - accuracy: 0.1250\n",
            "Epoch 56/500\n",
            "1/1 - 0s - loss: 3.0145 - accuracy: 0.1250\n",
            "Epoch 57/500\n",
            "1/1 - 0s - loss: 3.0117 - accuracy: 0.1250\n",
            "Epoch 58/500\n",
            "1/1 - 0s - loss: 3.0088 - accuracy: 0.1250\n",
            "Epoch 59/500\n",
            "1/1 - 0s - loss: 3.0058 - accuracy: 0.1250\n",
            "Epoch 60/500\n",
            "1/1 - 0s - loss: 3.0028 - accuracy: 0.1250\n",
            "Epoch 61/500\n",
            "1/1 - 0s - loss: 2.9996 - accuracy: 0.1250\n",
            "Epoch 62/500\n",
            "1/1 - 0s - loss: 2.9964 - accuracy: 0.1250\n",
            "Epoch 63/500\n",
            "1/1 - 0s - loss: 2.9930 - accuracy: 0.1250\n",
            "Epoch 64/500\n",
            "1/1 - 0s - loss: 2.9896 - accuracy: 0.1250\n",
            "Epoch 65/500\n",
            "1/1 - 0s - loss: 2.9860 - accuracy: 0.1250\n",
            "Epoch 66/500\n",
            "1/1 - 0s - loss: 2.9824 - accuracy: 0.1250\n",
            "Epoch 67/500\n",
            "1/1 - 0s - loss: 2.9786 - accuracy: 0.1250\n",
            "Epoch 68/500\n",
            "1/1 - 0s - loss: 2.9747 - accuracy: 0.1250\n",
            "Epoch 69/500\n",
            "1/1 - 0s - loss: 2.9708 - accuracy: 0.1250\n",
            "Epoch 70/500\n",
            "1/1 - 0s - loss: 2.9667 - accuracy: 0.1250\n",
            "Epoch 71/500\n",
            "1/1 - 0s - loss: 2.9624 - accuracy: 0.1250\n",
            "Epoch 72/500\n",
            "1/1 - 0s - loss: 2.9581 - accuracy: 0.1250\n",
            "Epoch 73/500\n",
            "1/1 - 0s - loss: 2.9536 - accuracy: 0.1250\n",
            "Epoch 74/500\n",
            "1/1 - 0s - loss: 2.9490 - accuracy: 0.1250\n",
            "Epoch 75/500\n",
            "1/1 - 0s - loss: 2.9443 - accuracy: 0.1250\n",
            "Epoch 76/500\n",
            "1/1 - 0s - loss: 2.9395 - accuracy: 0.1250\n",
            "Epoch 77/500\n",
            "1/1 - 0s - loss: 2.9345 - accuracy: 0.1250\n",
            "Epoch 78/500\n",
            "1/1 - 0s - loss: 2.9294 - accuracy: 0.1250\n",
            "Epoch 79/500\n",
            "1/1 - 0s - loss: 2.9241 - accuracy: 0.1250\n",
            "Epoch 80/500\n",
            "1/1 - 0s - loss: 2.9187 - accuracy: 0.1250\n",
            "Epoch 81/500\n",
            "1/1 - 0s - loss: 2.9132 - accuracy: 0.1250\n",
            "Epoch 82/500\n",
            "1/1 - 0s - loss: 2.9075 - accuracy: 0.1250\n",
            "Epoch 83/500\n",
            "1/1 - 0s - loss: 2.9017 - accuracy: 0.1250\n",
            "Epoch 84/500\n",
            "1/1 - 0s - loss: 2.8957 - accuracy: 0.1250\n",
            "Epoch 85/500\n",
            "1/1 - 0s - loss: 2.8895 - accuracy: 0.1250\n",
            "Epoch 86/500\n",
            "1/1 - 0s - loss: 2.8832 - accuracy: 0.1250\n",
            "Epoch 87/500\n",
            "1/1 - 0s - loss: 2.8768 - accuracy: 0.1250\n",
            "Epoch 88/500\n",
            "1/1 - 0s - loss: 2.8702 - accuracy: 0.1250\n",
            "Epoch 89/500\n",
            "1/1 - 0s - loss: 2.8634 - accuracy: 0.1250\n",
            "Epoch 90/500\n",
            "1/1 - 0s - loss: 2.8564 - accuracy: 0.1250\n",
            "Epoch 91/500\n",
            "1/1 - 0s - loss: 2.8493 - accuracy: 0.1250\n",
            "Epoch 92/500\n",
            "1/1 - 0s - loss: 2.8420 - accuracy: 0.1250\n",
            "Epoch 93/500\n",
            "1/1 - 0s - loss: 2.8346 - accuracy: 0.1250\n",
            "Epoch 94/500\n",
            "1/1 - 0s - loss: 2.8270 - accuracy: 0.1250\n",
            "Epoch 95/500\n",
            "1/1 - 0s - loss: 2.8192 - accuracy: 0.1250\n",
            "Epoch 96/500\n",
            "1/1 - 0s - loss: 2.8113 - accuracy: 0.1250\n",
            "Epoch 97/500\n",
            "1/1 - 0s - loss: 2.8031 - accuracy: 0.1250\n",
            "Epoch 98/500\n",
            "1/1 - 0s - loss: 2.7948 - accuracy: 0.1250\n",
            "Epoch 99/500\n",
            "1/1 - 0s - loss: 2.7864 - accuracy: 0.1250\n",
            "Epoch 100/500\n",
            "1/1 - 0s - loss: 2.7777 - accuracy: 0.1250\n",
            "Epoch 101/500\n",
            "1/1 - 0s - loss: 2.7689 - accuracy: 0.1250\n",
            "Epoch 102/500\n",
            "1/1 - 0s - loss: 2.7599 - accuracy: 0.2083\n",
            "Epoch 103/500\n",
            "1/1 - 0s - loss: 2.7507 - accuracy: 0.2083\n",
            "Epoch 104/500\n",
            "1/1 - 0s - loss: 2.7414 - accuracy: 0.2083\n",
            "Epoch 105/500\n",
            "1/1 - 0s - loss: 2.7319 - accuracy: 0.2083\n",
            "Epoch 106/500\n",
            "1/1 - 0s - loss: 2.7222 - accuracy: 0.2083\n",
            "Epoch 107/500\n",
            "1/1 - 0s - loss: 2.7124 - accuracy: 0.2083\n",
            "Epoch 108/500\n",
            "1/1 - 0s - loss: 2.7024 - accuracy: 0.2083\n",
            "Epoch 109/500\n",
            "1/1 - 0s - loss: 2.6922 - accuracy: 0.2083\n",
            "Epoch 110/500\n",
            "1/1 - 0s - loss: 2.6819 - accuracy: 0.2083\n",
            "Epoch 111/500\n",
            "1/1 - 0s - loss: 2.6714 - accuracy: 0.2500\n",
            "Epoch 112/500\n",
            "1/1 - 0s - loss: 2.6607 - accuracy: 0.2500\n",
            "Epoch 113/500\n",
            "1/1 - 0s - loss: 2.6499 - accuracy: 0.2500\n",
            "Epoch 114/500\n",
            "1/1 - 0s - loss: 2.6390 - accuracy: 0.2500\n",
            "Epoch 115/500\n",
            "1/1 - 0s - loss: 2.6279 - accuracy: 0.2500\n",
            "Epoch 116/500\n",
            "1/1 - 0s - loss: 2.6167 - accuracy: 0.2500\n",
            "Epoch 117/500\n",
            "1/1 - 0s - loss: 2.6053 - accuracy: 0.2917\n",
            "Epoch 118/500\n",
            "1/1 - 0s - loss: 2.5938 - accuracy: 0.2917\n",
            "Epoch 119/500\n",
            "1/1 - 0s - loss: 2.5821 - accuracy: 0.2917\n",
            "Epoch 120/500\n",
            "1/1 - 0s - loss: 2.5704 - accuracy: 0.2917\n",
            "Epoch 121/500\n",
            "1/1 - 0s - loss: 2.5585 - accuracy: 0.2917\n",
            "Epoch 122/500\n",
            "1/1 - 0s - loss: 2.5465 - accuracy: 0.2917\n",
            "Epoch 123/500\n",
            "1/1 - 0s - loss: 2.5344 - accuracy: 0.2917\n",
            "Epoch 124/500\n",
            "1/1 - 0s - loss: 2.5222 - accuracy: 0.2917\n",
            "Epoch 125/500\n",
            "1/1 - 0s - loss: 2.5099 - accuracy: 0.2917\n",
            "Epoch 126/500\n",
            "1/1 - 0s - loss: 2.4975 - accuracy: 0.2917\n",
            "Epoch 127/500\n",
            "1/1 - 0s - loss: 2.4850 - accuracy: 0.2917\n",
            "Epoch 128/500\n",
            "1/1 - 0s - loss: 2.4724 - accuracy: 0.2917\n",
            "Epoch 129/500\n",
            "1/1 - 0s - loss: 2.4597 - accuracy: 0.2917\n",
            "Epoch 130/500\n",
            "1/1 - 0s - loss: 2.4470 - accuracy: 0.2917\n",
            "Epoch 131/500\n",
            "1/1 - 0s - loss: 2.4342 - accuracy: 0.2917\n",
            "Epoch 132/500\n",
            "1/1 - 0s - loss: 2.4213 - accuracy: 0.2917\n",
            "Epoch 133/500\n",
            "1/1 - 0s - loss: 2.4084 - accuracy: 0.2917\n",
            "Epoch 134/500\n",
            "1/1 - 0s - loss: 2.3954 - accuracy: 0.2917\n",
            "Epoch 135/500\n",
            "1/1 - 0s - loss: 2.3824 - accuracy: 0.2917\n",
            "Epoch 136/500\n",
            "1/1 - 0s - loss: 2.3693 - accuracy: 0.2917\n",
            "Epoch 137/500\n",
            "1/1 - 0s - loss: 2.3562 - accuracy: 0.2917\n",
            "Epoch 138/500\n",
            "1/1 - 0s - loss: 2.3430 - accuracy: 0.3333\n",
            "Epoch 139/500\n",
            "1/1 - 0s - loss: 2.3298 - accuracy: 0.3750\n",
            "Epoch 140/500\n",
            "1/1 - 0s - loss: 2.3166 - accuracy: 0.4167\n",
            "Epoch 141/500\n",
            "1/1 - 0s - loss: 2.3033 - accuracy: 0.4167\n",
            "Epoch 142/500\n",
            "1/1 - 0s - loss: 2.2900 - accuracy: 0.4583\n",
            "Epoch 143/500\n",
            "1/1 - 0s - loss: 2.2767 - accuracy: 0.4583\n",
            "Epoch 144/500\n",
            "1/1 - 0s - loss: 2.2634 - accuracy: 0.4583\n",
            "Epoch 145/500\n",
            "1/1 - 0s - loss: 2.2500 - accuracy: 0.4583\n",
            "Epoch 146/500\n",
            "1/1 - 0s - loss: 2.2366 - accuracy: 0.4583\n",
            "Epoch 147/500\n",
            "1/1 - 0s - loss: 2.2233 - accuracy: 0.5000\n",
            "Epoch 148/500\n",
            "1/1 - 0s - loss: 2.2099 - accuracy: 0.5000\n",
            "Epoch 149/500\n",
            "1/1 - 0s - loss: 2.1965 - accuracy: 0.5000\n",
            "Epoch 150/500\n",
            "1/1 - 0s - loss: 2.1831 - accuracy: 0.5000\n",
            "Epoch 151/500\n",
            "1/1 - 0s - loss: 2.1697 - accuracy: 0.5000\n",
            "Epoch 152/500\n",
            "1/1 - 0s - loss: 2.1563 - accuracy: 0.5000\n",
            "Epoch 153/500\n",
            "1/1 - 0s - loss: 2.1428 - accuracy: 0.5000\n",
            "Epoch 154/500\n",
            "1/1 - 0s - loss: 2.1294 - accuracy: 0.5000\n",
            "Epoch 155/500\n",
            "1/1 - 0s - loss: 2.1160 - accuracy: 0.5000\n",
            "Epoch 156/500\n",
            "1/1 - 0s - loss: 2.1026 - accuracy: 0.5000\n",
            "Epoch 157/500\n",
            "1/1 - 0s - loss: 2.0892 - accuracy: 0.5000\n",
            "Epoch 158/500\n",
            "1/1 - 0s - loss: 2.0758 - accuracy: 0.5000\n",
            "Epoch 159/500\n",
            "1/1 - 0s - loss: 2.0624 - accuracy: 0.5000\n",
            "Epoch 160/500\n",
            "1/1 - 0s - loss: 2.0490 - accuracy: 0.5000\n",
            "Epoch 161/500\n",
            "1/1 - 0s - loss: 2.0356 - accuracy: 0.5000\n",
            "Epoch 162/500\n",
            "1/1 - 0s - loss: 2.0222 - accuracy: 0.5000\n",
            "Epoch 163/500\n",
            "1/1 - 0s - loss: 2.0088 - accuracy: 0.5000\n",
            "Epoch 164/500\n",
            "1/1 - 0s - loss: 1.9955 - accuracy: 0.5417\n",
            "Epoch 165/500\n",
            "1/1 - 0s - loss: 1.9821 - accuracy: 0.5417\n",
            "Epoch 166/500\n",
            "1/1 - 0s - loss: 1.9688 - accuracy: 0.5417\n",
            "Epoch 167/500\n",
            "1/1 - 0s - loss: 1.9555 - accuracy: 0.5417\n",
            "Epoch 168/500\n",
            "1/1 - 0s - loss: 1.9421 - accuracy: 0.5417\n",
            "Epoch 169/500\n",
            "1/1 - 0s - loss: 1.9288 - accuracy: 0.5417\n",
            "Epoch 170/500\n",
            "1/1 - 0s - loss: 1.9156 - accuracy: 0.5417\n",
            "Epoch 171/500\n",
            "1/1 - 0s - loss: 1.9023 - accuracy: 0.5417\n",
            "Epoch 172/500\n",
            "1/1 - 0s - loss: 1.8890 - accuracy: 0.5417\n",
            "Epoch 173/500\n",
            "1/1 - 0s - loss: 1.8758 - accuracy: 0.5833\n",
            "Epoch 174/500\n",
            "1/1 - 0s - loss: 1.8625 - accuracy: 0.5833\n",
            "Epoch 175/500\n",
            "1/1 - 0s - loss: 1.8493 - accuracy: 0.6250\n",
            "Epoch 176/500\n",
            "1/1 - 0s - loss: 1.8361 - accuracy: 0.6250\n",
            "Epoch 177/500\n",
            "1/1 - 0s - loss: 1.8230 - accuracy: 0.6250\n",
            "Epoch 178/500\n",
            "1/1 - 0s - loss: 1.8098 - accuracy: 0.6250\n",
            "Epoch 179/500\n",
            "1/1 - 0s - loss: 1.7966 - accuracy: 0.6250\n",
            "Epoch 180/500\n",
            "1/1 - 0s - loss: 1.7835 - accuracy: 0.6250\n",
            "Epoch 181/500\n",
            "1/1 - 0s - loss: 1.7704 - accuracy: 0.6250\n",
            "Epoch 182/500\n",
            "1/1 - 0s - loss: 1.7573 - accuracy: 0.6667\n",
            "Epoch 183/500\n",
            "1/1 - 0s - loss: 1.7442 - accuracy: 0.6667\n",
            "Epoch 184/500\n",
            "1/1 - 0s - loss: 1.7312 - accuracy: 0.6667\n",
            "Epoch 185/500\n",
            "1/1 - 0s - loss: 1.7181 - accuracy: 0.6667\n",
            "Epoch 186/500\n",
            "1/1 - 0s - loss: 1.7051 - accuracy: 0.6667\n",
            "Epoch 187/500\n",
            "1/1 - 0s - loss: 1.6921 - accuracy: 0.7083\n",
            "Epoch 188/500\n",
            "1/1 - 0s - loss: 1.6791 - accuracy: 0.7083\n",
            "Epoch 189/500\n",
            "1/1 - 0s - loss: 1.6661 - accuracy: 0.7500\n",
            "Epoch 190/500\n",
            "1/1 - 0s - loss: 1.6532 - accuracy: 0.7500\n",
            "Epoch 191/500\n",
            "1/1 - 0s - loss: 1.6402 - accuracy: 0.7500\n",
            "Epoch 192/500\n",
            "1/1 - 0s - loss: 1.6273 - accuracy: 0.7500\n",
            "Epoch 193/500\n",
            "1/1 - 0s - loss: 1.6144 - accuracy: 0.7500\n",
            "Epoch 194/500\n",
            "1/1 - 0s - loss: 1.6015 - accuracy: 0.7500\n",
            "Epoch 195/500\n",
            "1/1 - 0s - loss: 1.5886 - accuracy: 0.7500\n",
            "Epoch 196/500\n",
            "1/1 - 0s - loss: 1.5758 - accuracy: 0.7500\n",
            "Epoch 197/500\n",
            "1/1 - 0s - loss: 1.5630 - accuracy: 0.7500\n",
            "Epoch 198/500\n",
            "1/1 - 0s - loss: 1.5501 - accuracy: 0.7500\n",
            "Epoch 199/500\n",
            "1/1 - 0s - loss: 1.5374 - accuracy: 0.7500\n",
            "Epoch 200/500\n",
            "1/1 - 0s - loss: 1.5246 - accuracy: 0.7917\n",
            "Epoch 201/500\n",
            "1/1 - 0s - loss: 1.5118 - accuracy: 0.7917\n",
            "Epoch 202/500\n",
            "1/1 - 0s - loss: 1.4991 - accuracy: 0.7917\n",
            "Epoch 203/500\n",
            "1/1 - 0s - loss: 1.4864 - accuracy: 0.7917\n",
            "Epoch 204/500\n",
            "1/1 - 0s - loss: 1.4737 - accuracy: 0.7917\n",
            "Epoch 205/500\n",
            "1/1 - 0s - loss: 1.4611 - accuracy: 0.7917\n",
            "Epoch 206/500\n",
            "1/1 - 0s - loss: 1.4484 - accuracy: 0.7917\n",
            "Epoch 207/500\n",
            "1/1 - 0s - loss: 1.4358 - accuracy: 0.7917\n",
            "Epoch 208/500\n",
            "1/1 - 0s - loss: 1.4232 - accuracy: 0.8333\n",
            "Epoch 209/500\n",
            "1/1 - 0s - loss: 1.4107 - accuracy: 0.8333\n",
            "Epoch 210/500\n",
            "1/1 - 0s - loss: 1.3981 - accuracy: 0.8333\n",
            "Epoch 211/500\n",
            "1/1 - 0s - loss: 1.3856 - accuracy: 0.8333\n",
            "Epoch 212/500\n",
            "1/1 - 0s - loss: 1.3732 - accuracy: 0.8333\n",
            "Epoch 213/500\n",
            "1/1 - 0s - loss: 1.3607 - accuracy: 0.8333\n",
            "Epoch 214/500\n",
            "1/1 - 0s - loss: 1.3483 - accuracy: 0.8333\n",
            "Epoch 215/500\n",
            "1/1 - 0s - loss: 1.3360 - accuracy: 0.8333\n",
            "Epoch 216/500\n",
            "1/1 - 0s - loss: 1.3236 - accuracy: 0.8333\n",
            "Epoch 217/500\n",
            "1/1 - 0s - loss: 1.3113 - accuracy: 0.8750\n",
            "Epoch 218/500\n",
            "1/1 - 0s - loss: 1.2991 - accuracy: 0.8750\n",
            "Epoch 219/500\n",
            "1/1 - 0s - loss: 1.2869 - accuracy: 0.8750\n",
            "Epoch 220/500\n",
            "1/1 - 0s - loss: 1.2747 - accuracy: 0.8750\n",
            "Epoch 221/500\n",
            "1/1 - 0s - loss: 1.2626 - accuracy: 0.8750\n",
            "Epoch 222/500\n",
            "1/1 - 0s - loss: 1.2505 - accuracy: 0.8750\n",
            "Epoch 223/500\n",
            "1/1 - 0s - loss: 1.2384 - accuracy: 0.8750\n",
            "Epoch 224/500\n",
            "1/1 - 0s - loss: 1.2264 - accuracy: 0.8750\n",
            "Epoch 225/500\n",
            "1/1 - 0s - loss: 1.2145 - accuracy: 0.8750\n",
            "Epoch 226/500\n",
            "1/1 - 0s - loss: 1.2026 - accuracy: 0.8750\n",
            "Epoch 227/500\n",
            "1/1 - 0s - loss: 1.1908 - accuracy: 0.8750\n",
            "Epoch 228/500\n",
            "1/1 - 0s - loss: 1.1790 - accuracy: 0.8750\n",
            "Epoch 229/500\n",
            "1/1 - 0s - loss: 1.1673 - accuracy: 0.8750\n",
            "Epoch 230/500\n",
            "1/1 - 0s - loss: 1.1556 - accuracy: 0.8750\n",
            "Epoch 231/500\n",
            "1/1 - 0s - loss: 1.1440 - accuracy: 0.8750\n",
            "Epoch 232/500\n",
            "1/1 - 0s - loss: 1.1325 - accuracy: 0.8750\n",
            "Epoch 233/500\n",
            "1/1 - 0s - loss: 1.1210 - accuracy: 0.8750\n",
            "Epoch 234/500\n",
            "1/1 - 0s - loss: 1.1096 - accuracy: 0.8750\n",
            "Epoch 235/500\n",
            "1/1 - 0s - loss: 1.0982 - accuracy: 0.8750\n",
            "Epoch 236/500\n",
            "1/1 - 0s - loss: 1.0870 - accuracy: 0.8750\n",
            "Epoch 237/500\n",
            "1/1 - 0s - loss: 1.0758 - accuracy: 0.8750\n",
            "Epoch 238/500\n",
            "1/1 - 0s - loss: 1.0647 - accuracy: 0.8750\n",
            "Epoch 239/500\n",
            "1/1 - 0s - loss: 1.0536 - accuracy: 0.8750\n",
            "Epoch 240/500\n",
            "1/1 - 0s - loss: 1.0427 - accuracy: 0.8750\n",
            "Epoch 241/500\n",
            "1/1 - 0s - loss: 1.0318 - accuracy: 0.8750\n",
            "Epoch 242/500\n",
            "1/1 - 0s - loss: 1.0210 - accuracy: 0.8750\n",
            "Epoch 243/500\n",
            "1/1 - 0s - loss: 1.0102 - accuracy: 0.8750\n",
            "Epoch 244/500\n",
            "1/1 - 0s - loss: 0.9996 - accuracy: 0.8750\n",
            "Epoch 245/500\n",
            "1/1 - 0s - loss: 0.9891 - accuracy: 0.8750\n",
            "Epoch 246/500\n",
            "1/1 - 0s - loss: 0.9786 - accuracy: 0.8750\n",
            "Epoch 247/500\n",
            "1/1 - 0s - loss: 0.9682 - accuracy: 0.8750\n",
            "Epoch 248/500\n",
            "1/1 - 0s - loss: 0.9579 - accuracy: 0.8750\n",
            "Epoch 249/500\n",
            "1/1 - 0s - loss: 0.9477 - accuracy: 0.8750\n",
            "Epoch 250/500\n",
            "1/1 - 0s - loss: 0.9376 - accuracy: 0.8750\n",
            "Epoch 251/500\n",
            "1/1 - 0s - loss: 0.9276 - accuracy: 0.8750\n",
            "Epoch 252/500\n",
            "1/1 - 0s - loss: 0.9177 - accuracy: 0.8750\n",
            "Epoch 253/500\n",
            "1/1 - 0s - loss: 0.9079 - accuracy: 0.8750\n",
            "Epoch 254/500\n",
            "1/1 - 0s - loss: 0.8982 - accuracy: 0.8750\n",
            "Epoch 255/500\n",
            "1/1 - 0s - loss: 0.8885 - accuracy: 0.8750\n",
            "Epoch 256/500\n",
            "1/1 - 0s - loss: 0.8790 - accuracy: 0.8750\n",
            "Epoch 257/500\n",
            "1/1 - 0s - loss: 0.8696 - accuracy: 0.8750\n",
            "Epoch 258/500\n",
            "1/1 - 0s - loss: 0.8602 - accuracy: 0.8750\n",
            "Epoch 259/500\n",
            "1/1 - 0s - loss: 0.8510 - accuracy: 0.8750\n",
            "Epoch 260/500\n",
            "1/1 - 0s - loss: 0.8419 - accuracy: 0.8750\n",
            "Epoch 261/500\n",
            "1/1 - 0s - loss: 0.8328 - accuracy: 0.8750\n",
            "Epoch 262/500\n",
            "1/1 - 0s - loss: 0.8239 - accuracy: 0.8750\n",
            "Epoch 263/500\n",
            "1/1 - 0s - loss: 0.8151 - accuracy: 0.8750\n",
            "Epoch 264/500\n",
            "1/1 - 0s - loss: 0.8064 - accuracy: 0.8750\n",
            "Epoch 265/500\n",
            "1/1 - 0s - loss: 0.7977 - accuracy: 0.8750\n",
            "Epoch 266/500\n",
            "1/1 - 0s - loss: 0.7892 - accuracy: 0.8750\n",
            "Epoch 267/500\n",
            "1/1 - 0s - loss: 0.7808 - accuracy: 0.8750\n",
            "Epoch 268/500\n",
            "1/1 - 0s - loss: 0.7725 - accuracy: 0.8750\n",
            "Epoch 269/500\n",
            "1/1 - 0s - loss: 0.7643 - accuracy: 0.8750\n",
            "Epoch 270/500\n",
            "1/1 - 0s - loss: 0.7562 - accuracy: 0.8750\n",
            "Epoch 271/500\n",
            "1/1 - 0s - loss: 0.7481 - accuracy: 0.8750\n",
            "Epoch 272/500\n",
            "1/1 - 0s - loss: 0.7402 - accuracy: 0.8750\n",
            "Epoch 273/500\n",
            "1/1 - 0s - loss: 0.7324 - accuracy: 0.8750\n",
            "Epoch 274/500\n",
            "1/1 - 0s - loss: 0.7247 - accuracy: 0.8750\n",
            "Epoch 275/500\n",
            "1/1 - 0s - loss: 0.7171 - accuracy: 0.8750\n",
            "Epoch 276/500\n",
            "1/1 - 0s - loss: 0.7096 - accuracy: 0.8750\n",
            "Epoch 277/500\n",
            "1/1 - 0s - loss: 0.7022 - accuracy: 0.8750\n",
            "Epoch 278/500\n",
            "1/1 - 0s - loss: 0.6949 - accuracy: 0.8750\n",
            "Epoch 279/500\n",
            "1/1 - 0s - loss: 0.6877 - accuracy: 0.8750\n",
            "Epoch 280/500\n",
            "1/1 - 0s - loss: 0.6806 - accuracy: 0.8750\n",
            "Epoch 281/500\n",
            "1/1 - 0s - loss: 0.6736 - accuracy: 0.8750\n",
            "Epoch 282/500\n",
            "1/1 - 0s - loss: 0.6667 - accuracy: 0.8750\n",
            "Epoch 283/500\n",
            "1/1 - 0s - loss: 0.6599 - accuracy: 0.8750\n",
            "Epoch 284/500\n",
            "1/1 - 0s - loss: 0.6532 - accuracy: 0.8750\n",
            "Epoch 285/500\n",
            "1/1 - 0s - loss: 0.6465 - accuracy: 0.8750\n",
            "Epoch 286/500\n",
            "1/1 - 0s - loss: 0.6400 - accuracy: 0.8750\n",
            "Epoch 287/500\n",
            "1/1 - 0s - loss: 0.6336 - accuracy: 0.8750\n",
            "Epoch 288/500\n",
            "1/1 - 0s - loss: 0.6272 - accuracy: 0.8750\n",
            "Epoch 289/500\n",
            "1/1 - 0s - loss: 0.6210 - accuracy: 0.8750\n",
            "Epoch 290/500\n",
            "1/1 - 0s - loss: 0.6148 - accuracy: 0.8750\n",
            "Epoch 291/500\n",
            "1/1 - 0s - loss: 0.6088 - accuracy: 0.8750\n",
            "Epoch 292/500\n",
            "1/1 - 0s - loss: 0.6028 - accuracy: 0.8750\n",
            "Epoch 293/500\n",
            "1/1 - 0s - loss: 0.5969 - accuracy: 0.8750\n",
            "Epoch 294/500\n",
            "1/1 - 0s - loss: 0.5911 - accuracy: 0.8750\n",
            "Epoch 295/500\n",
            "1/1 - 0s - loss: 0.5854 - accuracy: 0.8750\n",
            "Epoch 296/500\n",
            "1/1 - 0s - loss: 0.5798 - accuracy: 0.8750\n",
            "Epoch 297/500\n",
            "1/1 - 0s - loss: 0.5742 - accuracy: 0.8750\n",
            "Epoch 298/500\n",
            "1/1 - 0s - loss: 0.5688 - accuracy: 0.8750\n",
            "Epoch 299/500\n",
            "1/1 - 0s - loss: 0.5634 - accuracy: 0.8750\n",
            "Epoch 300/500\n",
            "1/1 - 0s - loss: 0.5581 - accuracy: 0.8750\n",
            "Epoch 301/500\n",
            "1/1 - 0s - loss: 0.5529 - accuracy: 0.8750\n",
            "Epoch 302/500\n",
            "1/1 - 0s - loss: 0.5478 - accuracy: 0.8750\n",
            "Epoch 303/500\n",
            "1/1 - 0s - loss: 0.5427 - accuracy: 0.8750\n",
            "Epoch 304/500\n",
            "1/1 - 0s - loss: 0.5378 - accuracy: 0.8750\n",
            "Epoch 305/500\n",
            "1/1 - 0s - loss: 0.5329 - accuracy: 0.8750\n",
            "Epoch 306/500\n",
            "1/1 - 0s - loss: 0.5281 - accuracy: 0.8750\n",
            "Epoch 307/500\n",
            "1/1 - 0s - loss: 0.5233 - accuracy: 0.8750\n",
            "Epoch 308/500\n",
            "1/1 - 0s - loss: 0.5187 - accuracy: 0.8750\n",
            "Epoch 309/500\n",
            "1/1 - 0s - loss: 0.5141 - accuracy: 0.8750\n",
            "Epoch 310/500\n",
            "1/1 - 0s - loss: 0.5095 - accuracy: 0.8750\n",
            "Epoch 311/500\n",
            "1/1 - 0s - loss: 0.5051 - accuracy: 0.8750\n",
            "Epoch 312/500\n",
            "1/1 - 0s - loss: 0.5007 - accuracy: 0.8750\n",
            "Epoch 313/500\n",
            "1/1 - 0s - loss: 0.4964 - accuracy: 0.8750\n",
            "Epoch 314/500\n",
            "1/1 - 0s - loss: 0.4922 - accuracy: 0.8750\n",
            "Epoch 315/500\n",
            "1/1 - 0s - loss: 0.4880 - accuracy: 0.8750\n",
            "Epoch 316/500\n",
            "1/1 - 0s - loss: 0.4839 - accuracy: 0.8750\n",
            "Epoch 317/500\n",
            "1/1 - 0s - loss: 0.4799 - accuracy: 0.8750\n",
            "Epoch 318/500\n",
            "1/1 - 0s - loss: 0.4759 - accuracy: 0.8750\n",
            "Epoch 319/500\n",
            "1/1 - 0s - loss: 0.4720 - accuracy: 0.8750\n",
            "Epoch 320/500\n",
            "1/1 - 0s - loss: 0.4681 - accuracy: 0.8750\n",
            "Epoch 321/500\n",
            "1/1 - 0s - loss: 0.4643 - accuracy: 0.8750\n",
            "Epoch 322/500\n",
            "1/1 - 0s - loss: 0.4606 - accuracy: 0.8750\n",
            "Epoch 323/500\n",
            "1/1 - 0s - loss: 0.4569 - accuracy: 0.8750\n",
            "Epoch 324/500\n",
            "1/1 - 0s - loss: 0.4533 - accuracy: 0.8750\n",
            "Epoch 325/500\n",
            "1/1 - 0s - loss: 0.4498 - accuracy: 0.8750\n",
            "Epoch 326/500\n",
            "1/1 - 0s - loss: 0.4463 - accuracy: 0.8750\n",
            "Epoch 327/500\n",
            "1/1 - 0s - loss: 0.4429 - accuracy: 0.8750\n",
            "Epoch 328/500\n",
            "1/1 - 0s - loss: 0.4395 - accuracy: 0.8750\n",
            "Epoch 329/500\n",
            "1/1 - 0s - loss: 0.4362 - accuracy: 0.8750\n",
            "Epoch 330/500\n",
            "1/1 - 0s - loss: 0.4329 - accuracy: 0.8750\n",
            "Epoch 331/500\n",
            "1/1 - 0s - loss: 0.4297 - accuracy: 0.8750\n",
            "Epoch 332/500\n",
            "1/1 - 0s - loss: 0.4265 - accuracy: 0.8750\n",
            "Epoch 333/500\n",
            "1/1 - 0s - loss: 0.4234 - accuracy: 0.8750\n",
            "Epoch 334/500\n",
            "1/1 - 0s - loss: 0.4204 - accuracy: 0.8750\n",
            "Epoch 335/500\n",
            "1/1 - 0s - loss: 0.4173 - accuracy: 0.8750\n",
            "Epoch 336/500\n",
            "1/1 - 0s - loss: 0.4144 - accuracy: 0.8750\n",
            "Epoch 337/500\n",
            "1/1 - 0s - loss: 0.4115 - accuracy: 0.8750\n",
            "Epoch 338/500\n",
            "1/1 - 0s - loss: 0.4086 - accuracy: 0.8750\n",
            "Epoch 339/500\n",
            "1/1 - 0s - loss: 0.4058 - accuracy: 0.8750\n",
            "Epoch 340/500\n",
            "1/1 - 0s - loss: 0.4030 - accuracy: 0.8750\n",
            "Epoch 341/500\n",
            "1/1 - 0s - loss: 0.4003 - accuracy: 0.8750\n",
            "Epoch 342/500\n",
            "1/1 - 0s - loss: 0.3976 - accuracy: 0.8750\n",
            "Epoch 343/500\n",
            "1/1 - 0s - loss: 0.3949 - accuracy: 0.8750\n",
            "Epoch 344/500\n",
            "1/1 - 0s - loss: 0.3923 - accuracy: 0.8750\n",
            "Epoch 345/500\n",
            "1/1 - 0s - loss: 0.3898 - accuracy: 0.8750\n",
            "Epoch 346/500\n",
            "1/1 - 0s - loss: 0.3873 - accuracy: 0.8750\n",
            "Epoch 347/500\n",
            "1/1 - 0s - loss: 0.3848 - accuracy: 0.8750\n",
            "Epoch 348/500\n",
            "1/1 - 0s - loss: 0.3824 - accuracy: 0.8750\n",
            "Epoch 349/500\n",
            "1/1 - 0s - loss: 0.3800 - accuracy: 0.8750\n",
            "Epoch 350/500\n",
            "1/1 - 0s - loss: 0.3776 - accuracy: 0.8750\n",
            "Epoch 351/500\n",
            "1/1 - 0s - loss: 0.3753 - accuracy: 0.8750\n",
            "Epoch 352/500\n",
            "1/1 - 0s - loss: 0.3730 - accuracy: 0.8750\n",
            "Epoch 353/500\n",
            "1/1 - 0s - loss: 0.3708 - accuracy: 0.8750\n",
            "Epoch 354/500\n",
            "1/1 - 0s - loss: 0.3685 - accuracy: 0.8750\n",
            "Epoch 355/500\n",
            "1/1 - 0s - loss: 0.3664 - accuracy: 0.8750\n",
            "Epoch 356/500\n",
            "1/1 - 0s - loss: 0.3642 - accuracy: 0.8750\n",
            "Epoch 357/500\n",
            "1/1 - 0s - loss: 0.3621 - accuracy: 0.8750\n",
            "Epoch 358/500\n",
            "1/1 - 0s - loss: 0.3600 - accuracy: 0.8750\n",
            "Epoch 359/500\n",
            "1/1 - 0s - loss: 0.3580 - accuracy: 0.8750\n",
            "Epoch 360/500\n",
            "1/1 - 0s - loss: 0.3560 - accuracy: 0.8750\n",
            "Epoch 361/500\n",
            "1/1 - 0s - loss: 0.3540 - accuracy: 0.8750\n",
            "Epoch 362/500\n",
            "1/1 - 0s - loss: 0.3520 - accuracy: 0.8750\n",
            "Epoch 363/500\n",
            "1/1 - 0s - loss: 0.3501 - accuracy: 0.8750\n",
            "Epoch 364/500\n",
            "1/1 - 0s - loss: 0.3482 - accuracy: 0.8750\n",
            "Epoch 365/500\n",
            "1/1 - 0s - loss: 0.3464 - accuracy: 0.8750\n",
            "Epoch 366/500\n",
            "1/1 - 0s - loss: 0.3445 - accuracy: 0.8750\n",
            "Epoch 367/500\n",
            "1/1 - 0s - loss: 0.3427 - accuracy: 0.8750\n",
            "Epoch 368/500\n",
            "1/1 - 0s - loss: 0.3410 - accuracy: 0.8750\n",
            "Epoch 369/500\n",
            "1/1 - 0s - loss: 0.3392 - accuracy: 0.8750\n",
            "Epoch 370/500\n",
            "1/1 - 0s - loss: 0.3375 - accuracy: 0.8750\n",
            "Epoch 371/500\n",
            "1/1 - 0s - loss: 0.3358 - accuracy: 0.8750\n",
            "Epoch 372/500\n",
            "1/1 - 0s - loss: 0.3341 - accuracy: 0.8750\n",
            "Epoch 373/500\n",
            "1/1 - 0s - loss: 0.3325 - accuracy: 0.8750\n",
            "Epoch 374/500\n",
            "1/1 - 0s - loss: 0.3309 - accuracy: 0.8750\n",
            "Epoch 375/500\n",
            "1/1 - 0s - loss: 0.3293 - accuracy: 0.8750\n",
            "Epoch 376/500\n",
            "1/1 - 0s - loss: 0.3277 - accuracy: 0.8750\n",
            "Epoch 377/500\n",
            "1/1 - 0s - loss: 0.3261 - accuracy: 0.8750\n",
            "Epoch 378/500\n",
            "1/1 - 0s - loss: 0.3246 - accuracy: 0.8750\n",
            "Epoch 379/500\n",
            "1/1 - 0s - loss: 0.3231 - accuracy: 0.8750\n",
            "Epoch 380/500\n",
            "1/1 - 0s - loss: 0.3216 - accuracy: 0.8750\n",
            "Epoch 381/500\n",
            "1/1 - 0s - loss: 0.3202 - accuracy: 0.8750\n",
            "Epoch 382/500\n",
            "1/1 - 0s - loss: 0.3187 - accuracy: 0.8750\n",
            "Epoch 383/500\n",
            "1/1 - 0s - loss: 0.3173 - accuracy: 0.8750\n",
            "Epoch 384/500\n",
            "1/1 - 0s - loss: 0.3159 - accuracy: 0.8750\n",
            "Epoch 385/500\n",
            "1/1 - 0s - loss: 0.3145 - accuracy: 0.8750\n",
            "Epoch 386/500\n",
            "1/1 - 0s - loss: 0.3132 - accuracy: 0.8750\n",
            "Epoch 387/500\n",
            "1/1 - 0s - loss: 0.3119 - accuracy: 0.8750\n",
            "Epoch 388/500\n",
            "1/1 - 0s - loss: 0.3105 - accuracy: 0.8750\n",
            "Epoch 389/500\n",
            "1/1 - 0s - loss: 0.3092 - accuracy: 0.8750\n",
            "Epoch 390/500\n",
            "1/1 - 0s - loss: 0.3080 - accuracy: 0.8750\n",
            "Epoch 391/500\n",
            "1/1 - 0s - loss: 0.3067 - accuracy: 0.8750\n",
            "Epoch 392/500\n",
            "1/1 - 0s - loss: 0.3055 - accuracy: 0.8750\n",
            "Epoch 393/500\n",
            "1/1 - 0s - loss: 0.3042 - accuracy: 0.8750\n",
            "Epoch 394/500\n",
            "1/1 - 0s - loss: 0.3030 - accuracy: 0.8750\n",
            "Epoch 395/500\n",
            "1/1 - 0s - loss: 0.3018 - accuracy: 0.8750\n",
            "Epoch 396/500\n",
            "1/1 - 0s - loss: 0.3007 - accuracy: 0.8750\n",
            "Epoch 397/500\n",
            "1/1 - 0s - loss: 0.2995 - accuracy: 0.8750\n",
            "Epoch 398/500\n",
            "1/1 - 0s - loss: 0.2984 - accuracy: 0.8750\n",
            "Epoch 399/500\n",
            "1/1 - 0s - loss: 0.2972 - accuracy: 0.8750\n",
            "Epoch 400/500\n",
            "1/1 - 0s - loss: 0.2961 - accuracy: 0.8750\n",
            "Epoch 401/500\n",
            "1/1 - 0s - loss: 0.2950 - accuracy: 0.8750\n",
            "Epoch 402/500\n",
            "1/1 - 0s - loss: 0.2940 - accuracy: 0.8750\n",
            "Epoch 403/500\n",
            "1/1 - 0s - loss: 0.2929 - accuracy: 0.8750\n",
            "Epoch 404/500\n",
            "1/1 - 0s - loss: 0.2919 - accuracy: 0.8750\n",
            "Epoch 405/500\n",
            "1/1 - 0s - loss: 0.2908 - accuracy: 0.8750\n",
            "Epoch 406/500\n",
            "1/1 - 0s - loss: 0.2898 - accuracy: 0.8750\n",
            "Epoch 407/500\n",
            "1/1 - 0s - loss: 0.2888 - accuracy: 0.8750\n",
            "Epoch 408/500\n",
            "1/1 - 0s - loss: 0.2878 - accuracy: 0.8750\n",
            "Epoch 409/500\n",
            "1/1 - 0s - loss: 0.2869 - accuracy: 0.8750\n",
            "Epoch 410/500\n",
            "1/1 - 0s - loss: 0.2859 - accuracy: 0.8750\n",
            "Epoch 411/500\n",
            "1/1 - 0s - loss: 0.2849 - accuracy: 0.8750\n",
            "Epoch 412/500\n",
            "1/1 - 0s - loss: 0.2840 - accuracy: 0.8750\n",
            "Epoch 413/500\n",
            "1/1 - 0s - loss: 0.2831 - accuracy: 0.8750\n",
            "Epoch 414/500\n",
            "1/1 - 0s - loss: 0.2822 - accuracy: 0.8750\n",
            "Epoch 415/500\n",
            "1/1 - 0s - loss: 0.2813 - accuracy: 0.8750\n",
            "Epoch 416/500\n",
            "1/1 - 0s - loss: 0.2804 - accuracy: 0.8750\n",
            "Epoch 417/500\n",
            "1/1 - 0s - loss: 0.2795 - accuracy: 0.8750\n",
            "Epoch 418/500\n",
            "1/1 - 0s - loss: 0.2787 - accuracy: 0.8750\n",
            "Epoch 419/500\n",
            "1/1 - 0s - loss: 0.2778 - accuracy: 0.8750\n",
            "Epoch 420/500\n",
            "1/1 - 0s - loss: 0.2770 - accuracy: 0.8750\n",
            "Epoch 421/500\n",
            "1/1 - 0s - loss: 0.2762 - accuracy: 0.8750\n",
            "Epoch 422/500\n",
            "1/1 - 0s - loss: 0.2754 - accuracy: 0.8750\n",
            "Epoch 423/500\n",
            "1/1 - 0s - loss: 0.2746 - accuracy: 0.8750\n",
            "Epoch 424/500\n",
            "1/1 - 0s - loss: 0.2738 - accuracy: 0.8750\n",
            "Epoch 425/500\n",
            "1/1 - 0s - loss: 0.2730 - accuracy: 0.8750\n",
            "Epoch 426/500\n",
            "1/1 - 0s - loss: 0.2722 - accuracy: 0.8750\n",
            "Epoch 427/500\n",
            "1/1 - 0s - loss: 0.2715 - accuracy: 0.8750\n",
            "Epoch 428/500\n",
            "1/1 - 0s - loss: 0.2707 - accuracy: 0.8750\n",
            "Epoch 429/500\n",
            "1/1 - 0s - loss: 0.2700 - accuracy: 0.8750\n",
            "Epoch 430/500\n",
            "1/1 - 0s - loss: 0.2693 - accuracy: 0.8750\n",
            "Epoch 431/500\n",
            "1/1 - 0s - loss: 0.2686 - accuracy: 0.8750\n",
            "Epoch 432/500\n",
            "1/1 - 0s - loss: 0.2679 - accuracy: 0.8750\n",
            "Epoch 433/500\n",
            "1/1 - 0s - loss: 0.2672 - accuracy: 0.8750\n",
            "Epoch 434/500\n",
            "1/1 - 0s - loss: 0.2665 - accuracy: 0.8750\n",
            "Epoch 435/500\n",
            "1/1 - 0s - loss: 0.2658 - accuracy: 0.8750\n",
            "Epoch 436/500\n",
            "1/1 - 0s - loss: 0.2651 - accuracy: 0.8750\n",
            "Epoch 437/500\n",
            "1/1 - 0s - loss: 0.2645 - accuracy: 0.8750\n",
            "Epoch 438/500\n",
            "1/1 - 0s - loss: 0.2638 - accuracy: 0.8750\n",
            "Epoch 439/500\n",
            "1/1 - 0s - loss: 0.2632 - accuracy: 0.8750\n",
            "Epoch 440/500\n",
            "1/1 - 0s - loss: 0.2625 - accuracy: 0.8750\n",
            "Epoch 441/500\n",
            "1/1 - 0s - loss: 0.2619 - accuracy: 0.8750\n",
            "Epoch 442/500\n",
            "1/1 - 0s - loss: 0.2613 - accuracy: 0.8750\n",
            "Epoch 443/500\n",
            "1/1 - 0s - loss: 0.2607 - accuracy: 0.8750\n",
            "Epoch 444/500\n",
            "1/1 - 0s - loss: 0.2601 - accuracy: 0.8750\n",
            "Epoch 445/500\n",
            "1/1 - 0s - loss: 0.2595 - accuracy: 0.8750\n",
            "Epoch 446/500\n",
            "1/1 - 0s - loss: 0.2589 - accuracy: 0.8750\n",
            "Epoch 447/500\n",
            "1/1 - 0s - loss: 0.2584 - accuracy: 0.8750\n",
            "Epoch 448/500\n",
            "1/1 - 0s - loss: 0.2578 - accuracy: 0.8750\n",
            "Epoch 449/500\n",
            "1/1 - 0s - loss: 0.2572 - accuracy: 0.8750\n",
            "Epoch 450/500\n",
            "1/1 - 0s - loss: 0.2567 - accuracy: 0.8750\n",
            "Epoch 451/500\n",
            "1/1 - 0s - loss: 0.2561 - accuracy: 0.8750\n",
            "Epoch 452/500\n",
            "1/1 - 0s - loss: 0.2556 - accuracy: 0.8750\n",
            "Epoch 453/500\n",
            "1/1 - 0s - loss: 0.2551 - accuracy: 0.8750\n",
            "Epoch 454/500\n",
            "1/1 - 0s - loss: 0.2545 - accuracy: 0.8750\n",
            "Epoch 455/500\n",
            "1/1 - 0s - loss: 0.2540 - accuracy: 0.8750\n",
            "Epoch 456/500\n",
            "1/1 - 0s - loss: 0.2535 - accuracy: 0.8750\n",
            "Epoch 457/500\n",
            "1/1 - 0s - loss: 0.2530 - accuracy: 0.8750\n",
            "Epoch 458/500\n",
            "1/1 - 0s - loss: 0.2525 - accuracy: 0.8750\n",
            "Epoch 459/500\n",
            "1/1 - 0s - loss: 0.2520 - accuracy: 0.8750\n",
            "Epoch 460/500\n",
            "1/1 - 0s - loss: 0.2515 - accuracy: 0.8750\n",
            "Epoch 461/500\n",
            "1/1 - 0s - loss: 0.2511 - accuracy: 0.8750\n",
            "Epoch 462/500\n",
            "1/1 - 0s - loss: 0.2506 - accuracy: 0.8750\n",
            "Epoch 463/500\n",
            "1/1 - 0s - loss: 0.2501 - accuracy: 0.8750\n",
            "Epoch 464/500\n",
            "1/1 - 0s - loss: 0.2497 - accuracy: 0.8750\n",
            "Epoch 465/500\n",
            "1/1 - 0s - loss: 0.2492 - accuracy: 0.8750\n",
            "Epoch 466/500\n",
            "1/1 - 0s - loss: 0.2488 - accuracy: 0.8750\n",
            "Epoch 467/500\n",
            "1/1 - 0s - loss: 0.2483 - accuracy: 0.8750\n",
            "Epoch 468/500\n",
            "1/1 - 0s - loss: 0.2479 - accuracy: 0.8750\n",
            "Epoch 469/500\n",
            "1/1 - 0s - loss: 0.2474 - accuracy: 0.8750\n",
            "Epoch 470/500\n",
            "1/1 - 0s - loss: 0.2470 - accuracy: 0.8750\n",
            "Epoch 471/500\n",
            "1/1 - 0s - loss: 0.2466 - accuracy: 0.8750\n",
            "Epoch 472/500\n",
            "1/1 - 0s - loss: 0.2462 - accuracy: 0.8750\n",
            "Epoch 473/500\n",
            "1/1 - 0s - loss: 0.2458 - accuracy: 0.8750\n",
            "Epoch 474/500\n",
            "1/1 - 0s - loss: 0.2454 - accuracy: 0.8750\n",
            "Epoch 475/500\n",
            "1/1 - 0s - loss: 0.2450 - accuracy: 0.8750\n",
            "Epoch 476/500\n",
            "1/1 - 0s - loss: 0.2446 - accuracy: 0.8750\n",
            "Epoch 477/500\n",
            "1/1 - 0s - loss: 0.2442 - accuracy: 0.8750\n",
            "Epoch 478/500\n",
            "1/1 - 0s - loss: 0.2438 - accuracy: 0.8750\n",
            "Epoch 479/500\n",
            "1/1 - 0s - loss: 0.2434 - accuracy: 0.8750\n",
            "Epoch 480/500\n",
            "1/1 - 0s - loss: 0.2430 - accuracy: 0.8750\n",
            "Epoch 481/500\n",
            "1/1 - 0s - loss: 0.2427 - accuracy: 0.8750\n",
            "Epoch 482/500\n",
            "1/1 - 0s - loss: 0.2423 - accuracy: 0.8750\n",
            "Epoch 483/500\n",
            "1/1 - 0s - loss: 0.2419 - accuracy: 0.8750\n",
            "Epoch 484/500\n",
            "1/1 - 0s - loss: 0.2416 - accuracy: 0.8750\n",
            "Epoch 485/500\n",
            "1/1 - 0s - loss: 0.2412 - accuracy: 0.8750\n",
            "Epoch 486/500\n",
            "1/1 - 0s - loss: 0.2409 - accuracy: 0.8750\n",
            "Epoch 487/500\n",
            "1/1 - 0s - loss: 0.2405 - accuracy: 0.8750\n",
            "Epoch 488/500\n",
            "1/1 - 0s - loss: 0.2402 - accuracy: 0.8750\n",
            "Epoch 489/500\n",
            "1/1 - 0s - loss: 0.2398 - accuracy: 0.8750\n",
            "Epoch 490/500\n",
            "1/1 - 0s - loss: 0.2395 - accuracy: 0.8750\n",
            "Epoch 491/500\n",
            "1/1 - 0s - loss: 0.2392 - accuracy: 0.8750\n",
            "Epoch 492/500\n",
            "1/1 - 0s - loss: 0.2389 - accuracy: 0.8750\n",
            "Epoch 493/500\n",
            "1/1 - 0s - loss: 0.2385 - accuracy: 0.8750\n",
            "Epoch 494/500\n",
            "1/1 - 0s - loss: 0.2382 - accuracy: 0.8750\n",
            "Epoch 495/500\n",
            "1/1 - 0s - loss: 0.2379 - accuracy: 0.8750\n",
            "Epoch 496/500\n",
            "1/1 - 0s - loss: 0.2376 - accuracy: 0.8750\n",
            "Epoch 497/500\n",
            "1/1 - 0s - loss: 0.2373 - accuracy: 0.8750\n",
            "Epoch 498/500\n",
            "1/1 - 0s - loss: 0.2370 - accuracy: 0.8750\n",
            "Epoch 499/500\n",
            "1/1 - 0s - loss: 0.2367 - accuracy: 0.8750\n",
            "Epoch 500/500\n",
            "1/1 - 0s - loss: 0.2364 - accuracy: 0.8750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe48d049b90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra6ETLsF_X-N",
        "outputId": "9fe66c9c-e93a-451b-f437-4c869ce2ffec"
      },
      "source": [
        "in_text = 'Jill'\n",
        "print(in_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jill\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smdEGP03_X-N",
        "outputId": "ccd8d203-eb1a-4e21-ef1d-deb431a9b8b0"
      },
      "source": [
        "encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "encoded = array(encoded)\n",
        "yhat = model.predict_classes(encoded, verbose=0)\n",
        "yhat\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([19])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLhDxDoe_X-N",
        "outputId": "1e765656-e0e7-42f5-83a0-3339982858f0"
      },
      "source": [
        "for word, index in tokenizer.word_index.items():\n",
        "    if index == yhat:\n",
        "        print(word)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "came\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t87SffvW_X-O",
        "outputId": "8ac58793-037b-4a72-9c25-0bc95d6cdec5"
      },
      "source": [
        "start = \"jill\"\n",
        "print(start)\n",
        "for i in range(20):\n",
        "    encoded = tokenizer.texts_to_sequences([start])[0]\n",
        "    encoded = array(encoded)\n",
        "    yhat = model.predict_classes(encoded, verbose=0)\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == yhat:\n",
        "            print(word)\n",
        "            start=word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jill\n",
            "came\n",
            "tumbling\n",
            "after\n",
            "up\n",
            "the\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "hill\n",
            "to\n",
            "fetch\n",
            "a\n",
            "pail\n",
            "of\n",
            "water\n",
            "jack\n",
            "and\n",
            "jill\n",
            "came\n",
            "tumbling\n",
            "after\n",
            "up\n",
            "the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2x0cIDh_X-O"
      },
      "source": [
        "in_text = 'Jill'\n",
        "encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "encoded = array(encoded)\n",
        "yhat = model.predict(encoded, verbose=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB89JU7__X-O",
        "outputId": "2fa0a9b4-a7ff-4874-87bc-ffc69c27e2ba"
      },
      "source": [
        "yhat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.1701693e-04, 9.6275710e-04, 3.3060987e-05, 6.1181240e-06,\n",
              "        4.8698300e-01, 1.8113984e-03, 6.8283764e-05, 1.0436386e-04,\n",
              "        1.4539186e-03, 3.5600108e-04, 2.4453704e-03, 4.9551730e-03,\n",
              "        2.1457444e-03, 7.7117684e-06, 1.6127506e-05, 4.9910182e-03,\n",
              "        5.5204587e-06, 5.3519109e-04, 1.9491582e-04, 4.8783365e-01,\n",
              "        4.9582692e-03, 1.5248551e-05]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuj2NPmg_X-P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnPtvLzZ_X-P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}